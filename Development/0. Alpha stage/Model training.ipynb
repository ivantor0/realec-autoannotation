{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Predicting Movie Reviews with BERT on TF Hub.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "j0a4mTk9o1Qg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Copyright 2019 Google Inc.\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dCpvgG0vwXAZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Predicting Movie Review Sentiment with BERT on TF Hub"
      ]
    },
    {
      "metadata": {
        "id": "xiYrZKaHwV81",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you’ve been following Natural Language Processing over the past year, you’ve probably heard of BERT: Bidirectional Encoder Representations from Transformers. It’s a neural network architecture designed by Google researchers that’s totally transformed what’s state-of-the-art for NLP tasks, like text classification, translation, summarization, and question answering.\n",
        "\n",
        "Now that BERT's been added to [TF Hub](https://www.tensorflow.org/hub) as a loadable module, it's easy(ish) to add into existing Tensorflow text pipelines. In an existing pipeline, BERT can replace text embedding layers like ELMO and GloVE. Alternatively, [finetuning](http://wiki.fast.ai/index.php/Fine_tuning) BERT can provide both an accuracy boost and faster training time in many cases.\n",
        "\n",
        "Here, we'll train a model to predict whether an IMDB movie review is positive or negative using BERT in Tensorflow with tf hub. Some code was adapted from [this colab notebook](https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb). Let's get started!"
      ]
    },
    {
      "metadata": {
        "id": "hsZvic2YxnTz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cp5wfXDx5SPH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In addition to the standard libraries we imported above, we'll need to install BERT's python package."
      ]
    },
    {
      "metadata": {
        "id": "jviywGyWyKsA",
        "colab_type": "code",
        "outputId": "b0031a28-db03-4674-cbd6-26189f97e31a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install bert-tensorflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hhbGEfwgdEtw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KVB3eOcjxxm1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below, we'll set an output directory location to store our model output and checkpoints. This can be a local directory, in which case you'd set OUTPUT_DIR to the name of the directory you'd like to create. If you're running this code in Google's hosted Colab, the directory won't persist after the Colab session ends.\n",
        "\n",
        "Alternatively, if you're a GCP user, you can store output in a GCP bucket. To do that, set a directory name in OUTPUT_DIR and the name of the GCP bucket in the BUCKET field.\n",
        "\n",
        "Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist)."
      ]
    },
    {
      "metadata": {
        "id": "US_EAnICvP7f",
        "colab_type": "code",
        "outputId": "daa9f8c3-bf88-4280-994b-3ebad9d45009",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Set the output directory for saving model file\n",
        "# Optionally, set a GCP bucket location\n",
        "\n",
        "OUTPUT_DIR = 'OUTPUT_DIR_NAME'#@param {type:\"string\"}\n",
        "#@markdown Whether or not to clear/delete the directory and create a new one\n",
        "DO_DELETE = True #@param {type:\"boolean\"}\n",
        "#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n",
        "USE_BUCKET = False #@param {type:\"boolean\"}\n",
        "BUCKET = 'BUCKET_NAME' #@param {type:\"string\"}\n",
        "\n",
        "if USE_BUCKET:\n",
        "  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "\n",
        "if DO_DELETE:\n",
        "  try:\n",
        "    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
        "  except:\n",
        "    # Doesn't matter if the directory didn't exist\n",
        "    pass\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Model output directory: OUTPUT_DIR_NAME *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pmFYvkylMwXn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Data"
      ]
    },
    {
      "metadata": {
        "id": "MC_w8SRqN0fr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, let's download the dataset, hosted by Stanford. The code below, which downloads, extracts, and imports the IMDB Large Movie Review Dataset, is borrowed from [this Tensorflow tutorial](https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub)."
      ]
    },
    {
      "metadata": {
        "id": "fom_ff20gyy6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import os\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8_mKpp3RgY5z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trn = pd.read_json(\"https://github.com/ivantor0/realec-autoannotation/blob/master/gh_train.json?raw=true\").reset_index(drop=True)\n",
        "tst = pd.read_json(\"https://github.com/ivantor0/realec-autoannotation/blob/master/gh_test.json?raw=true\").reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wBe3GK5Rhja7",
        "colab_type": "code",
        "outputId": "b99ca659-ac6f-4eb4-ade9-28ecb9c17f99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1882
        }
      },
      "cell_type": "code",
      "source": [
        "trn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>entry</th>\n",
              "      <th>is_error</th>\n",
              "      <th>substring</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Some people believe that social networks, incl...</td>\n",
              "      <td>0</td>\n",
              "      <td>way</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The table shows underground of London, Paris, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The chart below shows changes in the number of...</td>\n",
              "      <td>0</td>\n",
              "      <td>of</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>It is argued that the mood of the employees ha...</td>\n",
              "      <td>0</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>﻿ Nowadays rates of crime by young people are ...</td>\n",
              "      <td>0</td>\n",
              "      <td>way</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>It can be clearly seen from the chart that the...</td>\n",
              "      <td>0</td>\n",
              "      <td>of</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>In USA's graph we see that number of eBook mar...</td>\n",
              "      <td>0</td>\n",
              "      <td>is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>That ’ s why if people want to be not only hea...</td>\n",
              "      <td>0</td>\n",
              "      <td>disiases</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>The majority of people ( 50,1%) was children i...</td>\n",
              "      <td>0</td>\n",
              "      <td>steadily</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Dear Ms Lawrence I am writing to inform you to...</td>\n",
              "      <td>0</td>\n",
              "      <td>our</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>This essay will consider some causes of this p...</td>\n",
              "      <td>0</td>\n",
              "      <td>What</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Sports make us fit and healthy, but shopping c...</td>\n",
              "      <td>0</td>\n",
              "      <td>is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Another obvious data in the chart is that spor...</td>\n",
              "      <td>0</td>\n",
              "      <td>time</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>There is a common opinion that technology we u...</td>\n",
              "      <td>0</td>\n",
              "      <td>people</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>The pay of chemicals goods transported by road...</td>\n",
              "      <td>0</td>\n",
              "      <td>Europe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>The pie charts provide an information about di...</td>\n",
              "      <td>0</td>\n",
              "      <td>going</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Writing task 1. The charts below give informat...</td>\n",
              "      <td>0</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Especially, these days the areas which have ve...</td>\n",
              "      <td>0</td>\n",
              "      <td>of</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>As the result of this trip can effect on lives...</td>\n",
              "      <td>0</td>\n",
              "      <td>must</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>In Middle East in 2014 the percent of unemploy...</td>\n",
              "      <td>0</td>\n",
              "      <td>fell</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>The chart gives information about the percenta...</td>\n",
              "      <td>1</td>\n",
              "      <td>quantum</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>According to the numbers given, girls had less...</td>\n",
              "      <td>0</td>\n",
              "      <td>to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>But we can see rapid growth in population aged...</td>\n",
              "      <td>0</td>\n",
              "      <td>be</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>It is commoned believe that airplanes are one ...</td>\n",
              "      <td>0</td>\n",
              "      <td>kinds</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Firstly, it is common knowledge that you can f...</td>\n",
              "      <td>0</td>\n",
              "      <td>sources</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Public health is supposed to be one of the mos...</td>\n",
              "      <td>1</td>\n",
              "      <td>chief</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>This essay will express a point of disagreemen...</td>\n",
              "      <td>0</td>\n",
              "      <td>great</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>The chart below gives us information about the...</td>\n",
              "      <td>0</td>\n",
              "      <td>Latin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Moreover, Tokyo ’ s underground railway statio...</td>\n",
              "      <td>0</td>\n",
              "      <td>been</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>The most common is lack of time to speaking wi...</td>\n",
              "      <td>0</td>\n",
              "      <td>by</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15970</th>\n",
              "      <td>The number of children will decrease in 3 per ...</td>\n",
              "      <td>0</td>\n",
              "      <td>both</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15971</th>\n",
              "      <td>Exercises can be not very difficult and intere...</td>\n",
              "      <td>0</td>\n",
              "      <td>of</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15972</th>\n",
              "      <td>Because of the air pollution and global warmin...</td>\n",
              "      <td>0</td>\n",
              "      <td>legal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15973</th>\n",
              "      <td>6) Juda is known to have betrayed Jesus. 7) Gi...</td>\n",
              "      <td>0</td>\n",
              "      <td>of</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15974</th>\n",
              "      <td>Another option is the decline in the number of...</td>\n",
              "      <td>0</td>\n",
              "      <td>proportions</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15975</th>\n",
              "      <td>In 1940 the proportion of population aged 65 a...</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15976</th>\n",
              "      <td>﻿ Interpersonal relationships have traditional...</td>\n",
              "      <td>0</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15977</th>\n",
              "      <td>The chart compare changes in rate of unemploym...</td>\n",
              "      <td>0</td>\n",
              "      <td>the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15978</th>\n",
              "      <td>Firsly, Britain and American films are trendy....</td>\n",
              "      <td>0</td>\n",
              "      <td>films</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15979</th>\n",
              "      <td>Furthermore, the percentage of men who held po...</td>\n",
              "      <td>0</td>\n",
              "      <td>held</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15980</th>\n",
              "      <td>If this thinks county will buy, not produce, i...</td>\n",
              "      <td>0</td>\n",
              "      <td>is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15981</th>\n",
              "      <td>I totally disagree with this statement, but le...</td>\n",
              "      <td>0</td>\n",
              "      <td>with</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15982</th>\n",
              "      <td>The charts provide information on transportati...</td>\n",
              "      <td>0</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15983</th>\n",
              "      <td>However, I have another example, it consider w...</td>\n",
              "      <td>0</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15984</th>\n",
              "      <td>Air transport is one of the most harmful mean ...</td>\n",
              "      <td>1</td>\n",
              "      <td>raise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15985</th>\n",
              "      <td>So, will building sports facilities have desir...</td>\n",
              "      <td>0</td>\n",
              "      <td>morning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15986</th>\n",
              "      <td>[MASK] diagrams demonstrate the percentage of ...</td>\n",
              "      <td>0</td>\n",
              "      <td>The</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15987</th>\n",
              "      <td>The graph, which is offered to me, vividly rep...</td>\n",
              "      <td>0</td>\n",
              "      <td>the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15988</th>\n",
              "      <td>Secondly, a lot of developing countries are su...</td>\n",
              "      <td>0</td>\n",
              "      <td>not</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15989</th>\n",
              "      <td>In my opinion, this generally true but not alw...</td>\n",
              "      <td>0</td>\n",
              "      <td>improved</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15990</th>\n",
              "      <td>But despite all the changes that affect langua...</td>\n",
              "      <td>0</td>\n",
              "      <td>concerned</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15991</th>\n",
              "      <td>But there are a lot of outstanding and sometim...</td>\n",
              "      <td>0</td>\n",
              "      <td>And</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15992</th>\n",
              "      <td>Controversially, some people think that pupils...</td>\n",
              "      <td>0</td>\n",
              "      <td>some</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15993</th>\n",
              "      <td>Our members found herbal medicines very effect...</td>\n",
              "      <td>0</td>\n",
              "      <td>from</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15994</th>\n",
              "      <td>To crown it all it has to be said that the att...</td>\n",
              "      <td>0</td>\n",
              "      <td>with</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15995</th>\n",
              "      <td>The proportion of the elderers tends to have g...</td>\n",
              "      <td>0</td>\n",
              "      <td>was</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15996</th>\n",
              "      <td>He had a lot of criminal friends, missed a lot...</td>\n",
              "      <td>0</td>\n",
              "      <td>missed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15997</th>\n",
              "      <td>As can be seen, the large percentage of males ...</td>\n",
              "      <td>0</td>\n",
              "      <td>second</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15998</th>\n",
              "      <td>Of course, for long transfers we cannot use th...</td>\n",
              "      <td>0</td>\n",
              "      <td>of</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15999</th>\n",
              "      <td>The diagrams in the picture illustrates the si...</td>\n",
              "      <td>0</td>\n",
              "      <td>that</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   entry  is_error  \\\n",
              "0      Some people believe that social networks, incl...         0   \n",
              "1      The table shows underground of London, Paris, ...         0   \n",
              "2      The chart below shows changes in the number of...         0   \n",
              "3      It is argued that the mood of the employees ha...         0   \n",
              "4      ﻿ Nowadays rates of crime by young people are ...         0   \n",
              "5      It can be clearly seen from the chart that the...         0   \n",
              "6      In USA's graph we see that number of eBook mar...         0   \n",
              "7      That ’ s why if people want to be not only hea...         0   \n",
              "8      The majority of people ( 50,1%) was children i...         0   \n",
              "9      Dear Ms Lawrence I am writing to inform you to...         0   \n",
              "10     This essay will consider some causes of this p...         0   \n",
              "11     Sports make us fit and healthy, but shopping c...         0   \n",
              "12     Another obvious data in the chart is that spor...         0   \n",
              "13     There is a common opinion that technology we u...         0   \n",
              "14     The pay of chemicals goods transported by road...         0   \n",
              "15     The pie charts provide an information about di...         0   \n",
              "16     Writing task 1. The charts below give informat...         0   \n",
              "17     Especially, these days the areas which have ve...         0   \n",
              "18     As the result of this trip can effect on lives...         0   \n",
              "19     In Middle East in 2014 the percent of unemploy...         0   \n",
              "20     The chart gives information about the percenta...         1   \n",
              "21     According to the numbers given, girls had less...         0   \n",
              "22     But we can see rapid growth in population aged...         0   \n",
              "23     It is commoned believe that airplanes are one ...         0   \n",
              "24     Firstly, it is common knowledge that you can f...         0   \n",
              "25     Public health is supposed to be one of the mos...         1   \n",
              "26     This essay will express a point of disagreemen...         0   \n",
              "27     The chart below gives us information about the...         0   \n",
              "28     Moreover, Tokyo ’ s underground railway statio...         0   \n",
              "29     The most common is lack of time to speaking wi...         0   \n",
              "...                                                  ...       ...   \n",
              "15970  The number of children will decrease in 3 per ...         0   \n",
              "15971  Exercises can be not very difficult and intere...         0   \n",
              "15972  Because of the air pollution and global warmin...         0   \n",
              "15973  6) Juda is known to have betrayed Jesus. 7) Gi...         0   \n",
              "15974  Another option is the decline in the number of...         0   \n",
              "15975  In 1940 the proportion of population aged 65 a...         0   \n",
              "15976  ﻿ Interpersonal relationships have traditional...         0   \n",
              "15977  The chart compare changes in rate of unemploym...         0   \n",
              "15978  Firsly, Britain and American films are trendy....         0   \n",
              "15979  Furthermore, the percentage of men who held po...         0   \n",
              "15980  If this thinks county will buy, not produce, i...         0   \n",
              "15981  I totally disagree with this statement, but le...         0   \n",
              "15982  The charts provide information on transportati...         0   \n",
              "15983  However, I have another example, it consider w...         0   \n",
              "15984  Air transport is one of the most harmful mean ...         1   \n",
              "15985  So, will building sports facilities have desir...         0   \n",
              "15986  [MASK] diagrams demonstrate the percentage of ...         0   \n",
              "15987  The graph, which is offered to me, vividly rep...         0   \n",
              "15988  Secondly, a lot of developing countries are su...         0   \n",
              "15989  In my opinion, this generally true but not alw...         0   \n",
              "15990  But despite all the changes that affect langua...         0   \n",
              "15991  But there are a lot of outstanding and sometim...         0   \n",
              "15992  Controversially, some people think that pupils...         0   \n",
              "15993  Our members found herbal medicines very effect...         0   \n",
              "15994  To crown it all it has to be said that the att...         0   \n",
              "15995  The proportion of the elderers tends to have g...         0   \n",
              "15996  He had a lot of criminal friends, missed a lot...         0   \n",
              "15997  As can be seen, the large percentage of males ...         0   \n",
              "15998  Of course, for long transfers we cannot use th...         0   \n",
              "15999  The diagrams in the picture illustrates the si...         0   \n",
              "\n",
              "         substring  \n",
              "0              way  \n",
              "1               in  \n",
              "2               of  \n",
              "3              all  \n",
              "4              way  \n",
              "5               of  \n",
              "6               is  \n",
              "7         disiases  \n",
              "8         steadily  \n",
              "9              our  \n",
              "10            What  \n",
              "11              is  \n",
              "12            time  \n",
              "13          people  \n",
              "14          Europe  \n",
              "15           going  \n",
              "16              in  \n",
              "17              of  \n",
              "18            must  \n",
              "19            fell  \n",
              "20         quantum  \n",
              "21              to  \n",
              "22              be  \n",
              "23           kinds  \n",
              "24         sources  \n",
              "25           chief  \n",
              "26           great  \n",
              "27           Latin  \n",
              "28            been  \n",
              "29              by  \n",
              "...            ...  \n",
              "15970         both  \n",
              "15971           of  \n",
              "15972        legal  \n",
              "15973           of  \n",
              "15974  proportions  \n",
              "15975            a  \n",
              "15976           in  \n",
              "15977          the  \n",
              "15978        films  \n",
              "15979         held  \n",
              "15980           is  \n",
              "15981         with  \n",
              "15982           in  \n",
              "15983          and  \n",
              "15984        raise  \n",
              "15985      morning  \n",
              "15986          The  \n",
              "15987          the  \n",
              "15988          not  \n",
              "15989     improved  \n",
              "15990    concerned  \n",
              "15991          And  \n",
              "15992         some  \n",
              "15993         from  \n",
              "15994         with  \n",
              "15995          was  \n",
              "15996       missed  \n",
              "15997       second  \n",
              "15998           of  \n",
              "15999         that  \n",
              "\n",
              "[16000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "XA8WHJgzhIZf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To keep training fast, we'll take a sample of 5000 train and test examples, respectively."
      ]
    },
    {
      "metadata": {
        "id": "lw_F488eixTV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = trn.sample(10000)\n",
        "test = tst.sample(10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3QTqhz-NXmWi",
        "colab_type": "code",
        "outputId": "2b3a82c5-3fe0-468c-d364-35bdc8525e47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1882
        }
      },
      "cell_type": "code",
      "source": [
        "train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>entry</th>\n",
              "      <th>is_error</th>\n",
              "      <th>substring</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6488</th>\n",
              "      <td>In my opinion, the government should allow pir...</td>\n",
              "      <td>1</td>\n",
              "      <td>large</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7638</th>\n",
              "      <td>This paper is designed as follows. In the firs...</td>\n",
              "      <td>1</td>\n",
              "      <td>then</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5639</th>\n",
              "      <td>And what to do if you can ’ t get your money b...</td>\n",
              "      <td>0</td>\n",
              "      <td>Allfilms</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7044</th>\n",
              "      <td>Nowadays [MASK] competition between companies ...</td>\n",
              "      <td>0</td>\n",
              "      <td>the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12723</th>\n",
              "      <td>It reached its peak in 2012 when exceeded 100 ...</td>\n",
              "      <td>0</td>\n",
              "      <td>Until</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7973</th>\n",
              "      <td>2013 year is a key one. Apple ’ s profits fell...</td>\n",
              "      <td>0</td>\n",
              "      <td>last</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7764</th>\n",
              "      <td>The graph grows from 2000 to 2060. That means ...</td>\n",
              "      <td>0</td>\n",
              "      <td>to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2287</th>\n",
              "      <td>In other words, this family provokes the risin...</td>\n",
              "      <td>0</td>\n",
              "      <td>Carve</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10190</th>\n",
              "      <td>Moreover, we have briliant example, when in wa...</td>\n",
              "      <td>0</td>\n",
              "      <td>studing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7972</th>\n",
              "      <td>The chart also illustrates the gender and regi...</td>\n",
              "      <td>0</td>\n",
              "      <td>data</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1681</th>\n",
              "      <td>And I ’ m sure they will change it even more. ...</td>\n",
              "      <td>0</td>\n",
              "      <td>this</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12074</th>\n",
              "      <td>I guess it is mostly because this book is not ...</td>\n",
              "      <td>0</td>\n",
              "      <td>by</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3234</th>\n",
              "      <td>And I am firmly believe in that. Since the Med...</td>\n",
              "      <td>0</td>\n",
              "      <td>equall</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15763</th>\n",
              "      <td>I think that he gives too much his attention t...</td>\n",
              "      <td>0</td>\n",
              "      <td>book</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>700</th>\n",
              "      <td>However, some people won ’ t agree with me. Fi...</td>\n",
              "      <td>0</td>\n",
              "      <td>deserve</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3330</th>\n",
              "      <td>Public health is supposed to be one of the mos...</td>\n",
              "      <td>0</td>\n",
              "      <td>methods</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13924</th>\n",
              "      <td>A significant proportion of the latter offered...</td>\n",
              "      <td>0</td>\n",
              "      <td>of</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14609</th>\n",
              "      <td>The amount of fixed-line local calls ( in bill...</td>\n",
              "      <td>0</td>\n",
              "      <td>its</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13902</th>\n",
              "      <td>Including tongue-twisters into the language pr...</td>\n",
              "      <td>0</td>\n",
              "      <td>they</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12392</th>\n",
              "      <td>On the other hand, just as any criminal can ch...</td>\n",
              "      <td>0</td>\n",
              "      <td>are</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9215</th>\n",
              "      <td>Nowadays [MASK] ere serious environmental prob...</td>\n",
              "      <td>0</td>\n",
              "      <td>there</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6439</th>\n",
              "      <td>To choose a direction of activities regularly ...</td>\n",
              "      <td>0</td>\n",
              "      <td>warm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8710</th>\n",
              "      <td>Also it can be mentioned that the ranking of r...</td>\n",
              "      <td>0</td>\n",
              "      <td>is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13099</th>\n",
              "      <td>As it been stated, air travel has a huge influ...</td>\n",
              "      <td>0</td>\n",
              "      <td>cities</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8084</th>\n",
              "      <td>In the past females were simply not allowed to...</td>\n",
              "      <td>0</td>\n",
              "      <td>make</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1186</th>\n",
              "      <td>We have another situation in Italy: the number...</td>\n",
              "      <td>0</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13093</th>\n",
              "      <td>11) Mayans are thought to have believed in the...</td>\n",
              "      <td>0</td>\n",
              "      <td>to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>939</th>\n",
              "      <td>Because of that teachers in schools should als...</td>\n",
              "      <td>0</td>\n",
              "      <td>teachers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1928</th>\n",
              "      <td>4) It could be safer to obtain a balanced reme...</td>\n",
              "      <td>0</td>\n",
              "      <td>is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14355</th>\n",
              "      <td>For example, males in the age of 16-24 do exer...</td>\n",
              "      <td>0</td>\n",
              "      <td>As</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11182</th>\n",
              "      <td>You should be confident and be yourself. Repea...</td>\n",
              "      <td>0</td>\n",
              "      <td>quoting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11568</th>\n",
              "      <td>The cart illustrated how many girls [MASK] boy...</td>\n",
              "      <td>0</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15412</th>\n",
              "      <td>For example, last researches have shown that a...</td>\n",
              "      <td>0</td>\n",
              "      <td>many</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15922</th>\n",
              "      <td>On the other hand, \"tank would have \"armored o...</td>\n",
              "      <td>0</td>\n",
              "      <td>model</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14345</th>\n",
              "      <td>[MASK] line graph compares the amount of touri...</td>\n",
              "      <td>0</td>\n",
              "      <td>The</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>Dr. Pitt's is a leading expert in the field of...</td>\n",
              "      <td>0</td>\n",
              "      <td>that</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>401</th>\n",
              "      <td>Music and films are not usually associated wit...</td>\n",
              "      <td>0</td>\n",
              "      <td>ineffective</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2834</th>\n",
              "      <td>Moreover, people who take risks aremore prepar...</td>\n",
              "      <td>0</td>\n",
              "      <td>experienced.All</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10860</th>\n",
              "      <td>Nowadays there are several points of view abou...</td>\n",
              "      <td>0</td>\n",
              "      <td>my</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1059</th>\n",
              "      <td>Their pictures may not be clearly understanded...</td>\n",
              "      <td>0</td>\n",
              "      <td>to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11185</th>\n",
              "      <td>This new period wasmarked by the conversion of...</td>\n",
              "      <td>0</td>\n",
              "      <td>tamed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>713</th>\n",
              "      <td>However, there is another point of view. Some ...</td>\n",
              "      <td>0</td>\n",
              "      <td>doing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9020</th>\n",
              "      <td>They put a lot of effort to prohibit illegal c...</td>\n",
              "      <td>0</td>\n",
              "      <td>t</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12416</th>\n",
              "      <td>But as for me, it ’ s only western type of thi...</td>\n",
              "      <td>0</td>\n",
              "      <td>being</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6586</th>\n",
              "      <td>I suppose tgat there must be some age sectors ...</td>\n",
              "      <td>0</td>\n",
              "      <td>existing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12858</th>\n",
              "      <td>I think that reducing of number of flights wil...</td>\n",
              "      <td>0</td>\n",
              "      <td>spheres</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6903</th>\n",
              "      <td>As the main objective of phonetic study is to ...</td>\n",
              "      <td>0</td>\n",
              "      <td>devoicing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14375</th>\n",
              "      <td>This chart demonstrates proportions of men and...</td>\n",
              "      <td>0</td>\n",
              "      <td>s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9719</th>\n",
              "      <td>The number of girls who didn't can get a prima...</td>\n",
              "      <td>0</td>\n",
              "      <td>it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3368</th>\n",
              "      <td>The represented chart illustrates the unemploy...</td>\n",
              "      <td>0</td>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10676</th>\n",
              "      <td>The neat key change in this region is that it ...</td>\n",
              "      <td>0</td>\n",
              "      <td>of</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15003</th>\n",
              "      <td>Also, some kinds of sports might be used in tr...</td>\n",
              "      <td>0</td>\n",
              "      <td>health</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12321</th>\n",
              "      <td>Although, some people propose reading books an...</td>\n",
              "      <td>0</td>\n",
              "      <td>aircompanies</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11125</th>\n",
              "      <td>Moreover, the state of happiness might be base...</td>\n",
              "      <td>0</td>\n",
              "      <td>needs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1430</th>\n",
              "      <td>To make them continue writing songs and record...</td>\n",
              "      <td>0</td>\n",
              "      <td>musicians</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4742</th>\n",
              "      <td>But in the future some conditions was changing...</td>\n",
              "      <td>0</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12600</th>\n",
              "      <td>The other reason, that music, fillms, books ar...</td>\n",
              "      <td>0</td>\n",
              "      <td>library</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14143</th>\n",
              "      <td>Development of social networking gives people ...</td>\n",
              "      <td>0</td>\n",
              "      <td>the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6180</th>\n",
              "      <td>In Conclusion, only in two regions there was a...</td>\n",
              "      <td>0</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2502</th>\n",
              "      <td>We can even see that they detroyes their healt...</td>\n",
              "      <td>0</td>\n",
              "      <td>People</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   entry  is_error  \\\n",
              "6488   In my opinion, the government should allow pir...         1   \n",
              "7638   This paper is designed as follows. In the firs...         1   \n",
              "5639   And what to do if you can ’ t get your money b...         0   \n",
              "7044   Nowadays [MASK] competition between companies ...         0   \n",
              "12723  It reached its peak in 2012 when exceeded 100 ...         0   \n",
              "7973   2013 year is a key one. Apple ’ s profits fell...         0   \n",
              "7764   The graph grows from 2000 to 2060. That means ...         0   \n",
              "2287   In other words, this family provokes the risin...         0   \n",
              "10190  Moreover, we have briliant example, when in wa...         0   \n",
              "7972   The chart also illustrates the gender and regi...         0   \n",
              "1681   And I ’ m sure they will change it even more. ...         0   \n",
              "12074  I guess it is mostly because this book is not ...         0   \n",
              "3234   And I am firmly believe in that. Since the Med...         0   \n",
              "15763  I think that he gives too much his attention t...         0   \n",
              "700    However, some people won ’ t agree with me. Fi...         0   \n",
              "3330   Public health is supposed to be one of the mos...         0   \n",
              "13924  A significant proportion of the latter offered...         0   \n",
              "14609  The amount of fixed-line local calls ( in bill...         0   \n",
              "13902  Including tongue-twisters into the language pr...         0   \n",
              "12392  On the other hand, just as any criminal can ch...         0   \n",
              "9215   Nowadays [MASK] ere serious environmental prob...         0   \n",
              "6439   To choose a direction of activities regularly ...         0   \n",
              "8710   Also it can be mentioned that the ranking of r...         0   \n",
              "13099  As it been stated, air travel has a huge influ...         0   \n",
              "8084   In the past females were simply not allowed to...         0   \n",
              "1186   We have another situation in Italy: the number...         0   \n",
              "13093  11) Mayans are thought to have believed in the...         0   \n",
              "939    Because of that teachers in schools should als...         0   \n",
              "1928   4) It could be safer to obtain a balanced reme...         0   \n",
              "14355  For example, males in the age of 16-24 do exer...         0   \n",
              "...                                                  ...       ...   \n",
              "11182  You should be confident and be yourself. Repea...         0   \n",
              "11568  The cart illustrated how many girls [MASK] boy...         0   \n",
              "15412  For example, last researches have shown that a...         0   \n",
              "15922  On the other hand, \"tank would have \"armored o...         0   \n",
              "14345  [MASK] line graph compares the amount of touri...         0   \n",
              "1995   Dr. Pitt's is a leading expert in the field of...         0   \n",
              "401    Music and films are not usually associated wit...         0   \n",
              "2834   Moreover, people who take risks aremore prepar...         0   \n",
              "10860  Nowadays there are several points of view abou...         0   \n",
              "1059   Their pictures may not be clearly understanded...         0   \n",
              "11185  This new period wasmarked by the conversion of...         0   \n",
              "713    However, there is another point of view. Some ...         0   \n",
              "9020   They put a lot of effort to prohibit illegal c...         0   \n",
              "12416  But as for me, it ’ s only western type of thi...         0   \n",
              "6586   I suppose tgat there must be some age sectors ...         0   \n",
              "12858  I think that reducing of number of flights wil...         0   \n",
              "6903   As the main objective of phonetic study is to ...         0   \n",
              "14375  This chart demonstrates proportions of men and...         0   \n",
              "9719   The number of girls who didn't can get a prima...         0   \n",
              "3368   The represented chart illustrates the unemploy...         0   \n",
              "10676  The neat key change in this region is that it ...         0   \n",
              "15003  Also, some kinds of sports might be used in tr...         0   \n",
              "12321  Although, some people propose reading books an...         0   \n",
              "11125  Moreover, the state of happiness might be base...         0   \n",
              "1430   To make them continue writing songs and record...         0   \n",
              "4742   But in the future some conditions was changing...         0   \n",
              "12600  The other reason, that music, fillms, books ar...         0   \n",
              "14143  Development of social networking gives people ...         0   \n",
              "6180   In Conclusion, only in two regions there was a...         0   \n",
              "2502   We can even see that they detroyes their healt...         0   \n",
              "\n",
              "             substring  \n",
              "6488             large  \n",
              "7638              then  \n",
              "5639          Allfilms  \n",
              "7044               the  \n",
              "12723            Until  \n",
              "7973              last  \n",
              "7764                to  \n",
              "2287             Carve  \n",
              "10190          studing  \n",
              "7972              data  \n",
              "1681              this  \n",
              "12074               by  \n",
              "3234            equall  \n",
              "15763             book  \n",
              "700            deserve  \n",
              "3330           methods  \n",
              "13924               of  \n",
              "14609              its  \n",
              "13902             they  \n",
              "12392              are  \n",
              "9215             there  \n",
              "6439              warm  \n",
              "8710                is  \n",
              "13099           cities  \n",
              "8084              make  \n",
              "1186                in  \n",
              "13093               to  \n",
              "939           teachers  \n",
              "1928                is  \n",
              "14355               As  \n",
              "...                ...  \n",
              "11182          quoting  \n",
              "11568              and  \n",
              "15412             many  \n",
              "15922            model  \n",
              "14345              The  \n",
              "1995              that  \n",
              "401        ineffective  \n",
              "2834   experienced.All  \n",
              "10860               my  \n",
              "1059                to  \n",
              "11185            tamed  \n",
              "713              doing  \n",
              "9020                 t  \n",
              "12416            being  \n",
              "6586          existing  \n",
              "12858          spheres  \n",
              "6903         devoicing  \n",
              "14375                s  \n",
              "9719                it  \n",
              "3368               all  \n",
              "10676               of  \n",
              "15003           health  \n",
              "12321     aircompanies  \n",
              "11125            needs  \n",
              "1430         musicians  \n",
              "4742                in  \n",
              "12600          library  \n",
              "14143              the  \n",
              "6180                 S  \n",
              "2502            People  \n",
              "\n",
              "[10000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "prRQM8pDi8xI",
        "colab_type": "code",
        "outputId": "9b92d14f-1122-499f-ba56-87e7db5a22b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['entry', 'is_error', 'substring'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "sfRnHSz3iSXz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For us, our input data is the 'sentence' column and our label is the 'polarity' column (0, 1 for negative and positive, respecitvely)"
      ]
    },
    {
      "metadata": {
        "id": "IuMOGwFui4it",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DATA_COLUMN = 'entry'\n",
        "SUBSTR_COLUMN = 'substring'\n",
        "LABEL_COLUMN = 'is_error'\n",
        "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
        "label_list = [0, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V399W0rqNJ-Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing\n",
        "We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n",
        "\n",
        "- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n",
        "- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n",
        "- `label` is the label for our example, i.e. True, False"
      ]
    },
    {
      "metadata": {
        "id": "p9gEt5SmM6i6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
        "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = x[SUBSTR_COLUMN], \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = x[SUBSTR_COLUMN], \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H7fHpWRZYZpp",
        "colab_type": "code",
        "outputId": "3ec2e961-6cbe-49c4-a08c-8fe63b3a0fe8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1108
        }
      },
      "cell_type": "code",
      "source": [
        "train_InputExamples"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6488     <bert.run_classifier.InputExample object at 0x...\n",
              "7638     <bert.run_classifier.InputExample object at 0x...\n",
              "5639     <bert.run_classifier.InputExample object at 0x...\n",
              "7044     <bert.run_classifier.InputExample object at 0x...\n",
              "12723    <bert.run_classifier.InputExample object at 0x...\n",
              "7973     <bert.run_classifier.InputExample object at 0x...\n",
              "7764     <bert.run_classifier.InputExample object at 0x...\n",
              "2287     <bert.run_classifier.InputExample object at 0x...\n",
              "10190    <bert.run_classifier.InputExample object at 0x...\n",
              "7972     <bert.run_classifier.InputExample object at 0x...\n",
              "1681     <bert.run_classifier.InputExample object at 0x...\n",
              "12074    <bert.run_classifier.InputExample object at 0x...\n",
              "3234     <bert.run_classifier.InputExample object at 0x...\n",
              "15763    <bert.run_classifier.InputExample object at 0x...\n",
              "700      <bert.run_classifier.InputExample object at 0x...\n",
              "3330     <bert.run_classifier.InputExample object at 0x...\n",
              "13924    <bert.run_classifier.InputExample object at 0x...\n",
              "14609    <bert.run_classifier.InputExample object at 0x...\n",
              "13902    <bert.run_classifier.InputExample object at 0x...\n",
              "12392    <bert.run_classifier.InputExample object at 0x...\n",
              "9215     <bert.run_classifier.InputExample object at 0x...\n",
              "6439     <bert.run_classifier.InputExample object at 0x...\n",
              "8710     <bert.run_classifier.InputExample object at 0x...\n",
              "13099    <bert.run_classifier.InputExample object at 0x...\n",
              "8084     <bert.run_classifier.InputExample object at 0x...\n",
              "1186     <bert.run_classifier.InputExample object at 0x...\n",
              "13093    <bert.run_classifier.InputExample object at 0x...\n",
              "939      <bert.run_classifier.InputExample object at 0x...\n",
              "1928     <bert.run_classifier.InputExample object at 0x...\n",
              "14355    <bert.run_classifier.InputExample object at 0x...\n",
              "                               ...                        \n",
              "11182    <bert.run_classifier.InputExample object at 0x...\n",
              "11568    <bert.run_classifier.InputExample object at 0x...\n",
              "15412    <bert.run_classifier.InputExample object at 0x...\n",
              "15922    <bert.run_classifier.InputExample object at 0x...\n",
              "14345    <bert.run_classifier.InputExample object at 0x...\n",
              "1995     <bert.run_classifier.InputExample object at 0x...\n",
              "401      <bert.run_classifier.InputExample object at 0x...\n",
              "2834     <bert.run_classifier.InputExample object at 0x...\n",
              "10860    <bert.run_classifier.InputExample object at 0x...\n",
              "1059     <bert.run_classifier.InputExample object at 0x...\n",
              "11185    <bert.run_classifier.InputExample object at 0x...\n",
              "713      <bert.run_classifier.InputExample object at 0x...\n",
              "9020     <bert.run_classifier.InputExample object at 0x...\n",
              "12416    <bert.run_classifier.InputExample object at 0x...\n",
              "6586     <bert.run_classifier.InputExample object at 0x...\n",
              "12858    <bert.run_classifier.InputExample object at 0x...\n",
              "6903     <bert.run_classifier.InputExample object at 0x...\n",
              "14375    <bert.run_classifier.InputExample object at 0x...\n",
              "9719     <bert.run_classifier.InputExample object at 0x...\n",
              "3368     <bert.run_classifier.InputExample object at 0x...\n",
              "10676    <bert.run_classifier.InputExample object at 0x...\n",
              "15003    <bert.run_classifier.InputExample object at 0x...\n",
              "12321    <bert.run_classifier.InputExample object at 0x...\n",
              "11125    <bert.run_classifier.InputExample object at 0x...\n",
              "1430     <bert.run_classifier.InputExample object at 0x...\n",
              "4742     <bert.run_classifier.InputExample object at 0x...\n",
              "12600    <bert.run_classifier.InputExample object at 0x...\n",
              "14143    <bert.run_classifier.InputExample object at 0x...\n",
              "6180     <bert.run_classifier.InputExample object at 0x...\n",
              "2502     <bert.run_classifier.InputExample object at 0x...\n",
              "Length: 10000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "SCZWZtKxObjh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n",
        "\n",
        "\n",
        "1. Lowercase our text (if we're using a BERT lowercase model)\n",
        "2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
        "3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
        "4. Map our words to indexes using a vocab file that BERT provides\n",
        "5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n",
        "6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
        "\n",
        "Happily, we don't have to worry about most of these details.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "qMWiDtpyQSoU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To start, we'll need to load a vocabulary file and lowercasing information directly from the BERT tf hub module:"
      ]
    },
    {
      "metadata": {
        "id": "IhJSe0QHNG7U",
        "colab_type": "code",
        "outputId": "171b0f65-9678-4974-a666-e8a529d7380e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "cell_type": "code",
      "source": [
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "\n",
        "class NonMaskOmittingTokenizer(bert.tokenization.FullTokenizer):\n",
        "  def tokenize(self, text):\n",
        "    split_tokens = []\n",
        "    for token in self.basic_tokenizer.tokenize(text):\n",
        "      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
        "        split_tokens.append(sub_token)\n",
        "\n",
        "    for index, item in enumerate(split_tokens):\n",
        "      if index >= len(split_tokens)-2:\n",
        "        break\n",
        "      if item == '[' and split_tokens[index + 1] == 'mask' and split_tokens[index + 2] == ']':\n",
        "        split_tokens[index] = \"[MASK]\"\n",
        "        del split_tokens[index + 1]\n",
        "        del split_tokens[index + 1]\n",
        "\n",
        "    return split_tokens\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                            tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "  return NonMaskOmittingTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z4oFkhpZBDKm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Great--we just learned that the BERT model we're using expects lowercase data (that's what stored in tokenization_info[\"do_lower_case\"]) and we also loaded BERT's vocab file. We also created a tokenizer, which breaks words into word pieces:"
      ]
    },
    {
      "metadata": {
        "id": "dsBo6RCtQmwx",
        "colab_type": "code",
        "outputId": "9cf835e0-0e76-46d7-9479-6be5fa0e7de4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this',\n",
              " 'here',\n",
              " \"'\",\n",
              " 's',\n",
              " 'an',\n",
              " 'example',\n",
              " 'of',\n",
              " 'using',\n",
              " 'the',\n",
              " 'bert',\n",
              " 'token',\n",
              " '##izer']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "0OEzfFIt6GIc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands."
      ]
    },
    {
      "metadata": {
        "id": "LL5W8gEGRTAf",
        "colab_type": "code",
        "outputId": "60a576e0-34e4-4510-eed6-e3643bc4dac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1304
        }
      },
      "cell_type": "code",
      "source": [
        "# We'll set sequences to be at most 128 tokens long.\n",
        "MAX_SEQ_LENGTH = 128\n",
        "# Convert our train and test features to InputFeatures that BERT understands.\n",
        "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 10000\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] in my opinion , the government should allow pirate files in internet as there are many people who cannot buy the original discs and use internet to listen to music or to watch film . on the other hand , musicians can make money on concerts and producers can have money from selling merchandise . despite the fact that there are a large number of pirate copies , i think that it is not a [MASK] problem , because a lot of people still go to cinema and buy cd discs . for example , the sites like kin ##op ##ois ##k . ru show that a lot of people prefer to go to the cinema instead of watching film at home . so [SEP] large [SEP]\n",
            "INFO:tensorflow:input_ids: 101 1999 2026 5448 1010 1996 2231 2323 3499 11304 6764 1999 4274 2004 2045 2024 2116 2111 2040 3685 4965 1996 2434 15303 1998 2224 4274 2000 4952 2000 2189 2030 2000 3422 2143 1012 2006 1996 2060 2192 1010 5389 2064 2191 2769 2006 6759 1998 6443 2064 2031 2769 2013 4855 16359 1012 2750 1996 2755 2008 2045 2024 1037 2312 2193 1997 11304 4809 1010 1045 2228 2008 2009 2003 2025 1037 103 3291 1010 2138 1037 2843 1997 2111 2145 2175 2000 5988 1998 4965 3729 15303 1012 2005 2742 1010 1996 4573 2066 12631 7361 10054 2243 1012 21766 2265 2008 1037 2843 1997 2111 9544 2000 2175 2000 1996 5988 2612 1997 3666 2143 2012 2188 1012 2061 102 2312 102\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] this paper is designed as follows . in the first section we analyze the methodology behind corpus studies in music ##ology and provide an overview of existing tools . we [MASK] justify the development of our corpus and set design guidelines and goals . in the next section we describe the corpus structure : how music scores are represented and searched . we also comment on the interface and what capabilities of in - depth analysis are available through it . [SEP] then [SEP]\n",
            "INFO:tensorflow:input_ids: 101 2023 3259 2003 2881 2004 4076 1012 1999 1996 2034 2930 2057 17908 1996 16134 2369 13931 2913 1999 2189 6779 1998 3073 2019 19184 1997 4493 5906 1012 2057 103 16114 1996 2458 1997 2256 13931 1998 2275 2640 11594 1998 3289 1012 1999 1996 2279 2930 2057 6235 1996 13931 3252 1024 2129 2189 7644 2024 3421 1998 9022 1012 2057 2036 7615 2006 1996 8278 1998 2054 9859 1997 1999 1011 5995 4106 2024 2800 2083 2009 1012 102 2059 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] and what to do if you can ’ t get your money back ? also the quality of shared copies is always not so good . [MASK] and music are firstly made the audience to enjoy it . bad quality can turn all the pleasure you could got into nothing . but there is also a lot of people who thinks in another way . [SEP] all ##film ##s [SEP]\n",
            "INFO:tensorflow:input_ids: 101 1998 2054 2000 2079 2065 2017 2064 1521 1056 2131 2115 2769 2067 1029 2036 1996 3737 1997 4207 4809 2003 2467 2025 2061 2204 1012 103 1998 2189 2024 15847 2081 1996 4378 2000 5959 2009 1012 2919 3737 2064 2735 2035 1996 5165 2017 2071 2288 2046 2498 1012 2021 2045 2003 2036 1037 2843 1997 2111 2040 6732 1999 2178 2126 1012 102 2035 23665 2015 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] nowadays [MASK] competition between companies is becoming more and more high . therefore , they have to use different methods of maintaining competitive . one of them is moving the industries from developed countries to developing ones . [SEP] the [SEP]\n",
            "INFO:tensorflow:input_ids: 101 13367 103 2971 2090 3316 2003 3352 2062 1998 2062 2152 1012 3568 1010 2027 2031 2000 2224 2367 4725 1997 8498 6975 1012 2028 1997 2068 2003 3048 1996 6088 2013 2764 3032 2000 4975 3924 1012 102 1996 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] it reached its peak in 2012 when exceeded 100 * bn . it also can be seen from the bar chart that developed countries always invested in renewable energy significantly more money than developing countries ; for ins ##tan ##se , in 2006 , the first year of the period , the investment of the former was 50 * bn higher than of the latter . [MASK] 2008 developing countries were raising the investments , however in 2009 it fell . then , in 2010 and 2011 it soared and peaked at the level of apr ##ox ##imate ##ly 175 * bn . two last years were , again , the years of decrease . [SEP] until [SEP]\n",
            "INFO:tensorflow:input_ids: 101 2009 2584 2049 4672 1999 2262 2043 14872 2531 1008 24869 1012 2009 2036 2064 2022 2464 2013 1996 3347 3673 2008 2764 3032 2467 11241 1999 13918 2943 6022 2062 2769 2084 4975 3032 1025 2005 16021 5794 3366 1010 1999 2294 1010 1996 2034 2095 1997 1996 2558 1010 1996 5211 1997 1996 2280 2001 2753 1008 24869 3020 2084 1997 1996 3732 1012 103 2263 4975 3032 2020 6274 1996 10518 1010 2174 1999 2268 2009 3062 1012 2059 1010 1999 2230 1998 2249 2009 29127 1998 6601 2012 1996 2504 1997 19804 11636 21499 2135 12862 1008 24869 1012 2048 2197 2086 2020 1010 2153 1010 1996 2086 1997 9885 1012 102 2127 102 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:Writing example 0 of 10000\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] it usually claimed that aviation has bad impact on the environment . so some people are convinced that governments ought [MASK] take measures on reduction of the amount of air travel to stop air pollution and global dim ##ming . from my point of view , such environmental problems are crucial nowadays , however i believed that they should be solved in some . other ways , not by cutting down the flights . [SEP] to [SEP]\n",
            "INFO:tensorflow:input_ids: 101 2009 2788 3555 2008 5734 2038 2919 4254 2006 1996 4044 1012 2061 2070 2111 2024 6427 2008 6867 11276 103 2202 5761 2006 7312 1997 1996 3815 1997 2250 3604 2000 2644 2250 10796 1998 3795 11737 6562 1012 2013 2026 2391 1997 3193 1010 2107 4483 3471 2024 10232 13367 1010 2174 1045 3373 2008 2027 2323 2022 13332 1999 2070 1012 2060 3971 1010 2025 2011 6276 2091 1996 7599 1012 102 2000 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] the given charts illustrate facts about travelling to the uk and from the uk , and the most popular places among the uk citizens to visit . speaking about the first chart , it shows that after 1984 there was a noticeable rise of visits abroad by the uk residents , and by 1999 it [MASK] more than 50 million people per year . visits of the uk residents rose too , but not as much as the first feature : after 1989 it reached more than 20 million people . as for the second graph , it show 1999 statistics of most visited countries by the uk residents . [SEP] reached [SEP]\n",
            "INFO:tensorflow:input_ids: 101 1996 2445 6093 19141 8866 2055 8932 2000 1996 2866 1998 2013 1996 2866 1010 1998 1996 2087 2759 3182 2426 1996 2866 4480 2000 3942 1012 4092 2055 1996 2034 3673 1010 2009 3065 2008 2044 3118 2045 2001 1037 17725 4125 1997 7879 6917 2011 1996 2866 3901 1010 1998 2011 2639 2009 103 2062 2084 2753 2454 2111 2566 2095 1012 7879 1997 1996 2866 3901 3123 2205 1010 2021 2025 2004 2172 2004 1996 2034 3444 1024 2044 2960 2009 2584 2062 2084 2322 2454 2111 1012 2004 2005 1996 2117 10629 1010 2009 2265 2639 6747 1997 2087 4716 3032 2011 1996 2866 3901 1012 102 2584 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] as we have of people of this kind , the amount of uploaded information is really big . thereby , the citizen journalism is a new means of managing the information . this is a [MASK] strategy of collecting information and sharing with others . however , information it be ? to my mind , it is better when news is both fast and accurate . [SEP] fantastic [SEP]\n",
            "INFO:tensorflow:input_ids: 101 2004 2057 2031 1997 2111 1997 2023 2785 1010 1996 3815 1997 21345 2592 2003 2428 2502 1012 8558 1010 1996 6926 8083 2003 1037 2047 2965 1997 6605 1996 2592 1012 2023 2003 1037 103 5656 1997 9334 2592 1998 6631 2007 2500 1012 2174 1010 2592 2009 2022 1029 2000 2026 2568 1010 2009 2003 2488 2043 2739 2003 2119 3435 1998 8321 1012 102 10392 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] to conclude with , it is important to understand that each action , especially which is done in public should be regulated by law , based not only on the law - maker ' s believes and preferences , but also on common sen ##ce of moral and human values . nevertheless , there should be no chance to transform the regulation law into a weapon of unlimited ce ##nzo ##rs ##hip , which is as far as i can say the most dangerous and probably the most effective tool of the construct ##ining awful and malicious political regime . freedom [MASK] speech is a freedom to say , not to stay silent . [SEP] of [SEP]\n",
            "INFO:tensorflow:input_ids: 101 2000 16519 2007 1010 2009 2003 2590 2000 3305 2008 2169 2895 1010 2926 2029 2003 2589 1999 2270 2323 2022 12222 2011 2375 1010 2241 2025 2069 2006 1996 2375 1011 9338 1005 1055 7164 1998 18394 1010 2021 2036 2006 2691 12411 3401 1997 7191 1998 2529 5300 1012 6600 1010 2045 2323 2022 2053 3382 2000 10938 1996 7816 2375 2046 1037 5195 1997 14668 8292 25650 2869 5605 1010 2029 2003 2004 2521 2004 1045 2064 2360 1996 2087 4795 1998 2763 1996 2087 4621 6994 1997 1996 9570 24002 9643 1998 24391 2576 6939 1012 4071 103 4613 2003 1037 4071 2000 2360 1010 2025 2000 2994 4333 1012 102 1997 102 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] they may suffer from depression , social ph ##ob ##ies and some other childhood disorders . fur ##the ##more children may develop some personality disorders also . thus , nowadays this issue has sparked the heated debate and world is trying to [MASK] out the solutions . as regarding the fact , that outside environment is more essential for the bond between human and nature without realising the beauty of nature , our future generation will not able to respect them properly . apparently , many tree are knocked down and woods are cleaned out , resulting natural cal ##ami ##ty and this is due to in ##ef ##fi ##cie ##nt love and respect towards the environment . [SEP] find [SEP]\n",
            "INFO:tensorflow:input_ids: 101 2027 2089 9015 2013 6245 1010 2591 6887 16429 3111 1998 2070 2060 5593 10840 1012 6519 10760 5974 2336 2089 4503 2070 6180 10840 2036 1012 2947 1010 13367 2023 3277 2038 13977 1996 9685 5981 1998 2088 2003 2667 2000 103 2041 1996 7300 1012 2004 4953 1996 2755 1010 2008 2648 4044 2003 2062 6827 2005 1996 5416 2090 2529 1998 3267 2302 27504 1996 5053 1997 3267 1010 2256 2925 4245 2097 2025 2583 2000 4847 2068 7919 1012 4593 1010 2116 3392 2024 6573 2091 1998 5249 2024 12176 2041 1010 4525 3019 10250 10631 3723 1998 2023 2003 2349 2000 1999 12879 8873 23402 3372 2293 1998 4847 2875 1996 4044 1012 102 2424 102 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ccp5trMwRtmr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Creating a model\n",
        "\n",
        "Now that we've prepared our data, let's focus on building a model. `create_model` does just this below. First, it loads the BERT tf hub module again (this time to extract the computation graph). Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task (i.e. classifying whether a movie review is positive or negative). This strategy of using a mostly trained model is called [fine-tuning](http://wiki.fast.ai/index.php/Fine_tuning)."
      ]
    },
    {
      "metadata": {
        "id": "6o2a5ZIvRcJq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
        "                 num_labels):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "\n",
        "  bert_module = hub.Module(\n",
        "      BERT_MODEL_HUB,\n",
        "      trainable=True)\n",
        "  bert_inputs = dict(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids)\n",
        "  bert_outputs = bert_module(\n",
        "      inputs=bert_inputs,\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)\n",
        "\n",
        "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
        "  # Use \"sequence_outputs\" for token-level output.\n",
        "  output_layer = bert_outputs[\"pooled_output\"]\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  # Create our own layer to tune for politeness data.\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "\n",
        "    # Dropout helps prevent overfitting\n",
        "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    # Convert labels into one-hot encoding\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
        "    if is_predicting:\n",
        "      return (predicted_labels, log_probs)\n",
        "\n",
        "    # If we're train/eval, compute loss between predicted and actual label\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "    return (loss, predicted_labels, log_probs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qpE0ZIDOCQzE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction."
      ]
    },
    {
      "metadata": {
        "id": "FnH-AnOQ9KKW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model_fn_builder actually creates our model function\n",
        "# using the passed parameters for num_labels, learning_rate, etc.\n",
        "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
        "                     num_warmup_steps):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "\n",
        "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
        "    \n",
        "    # TRAIN and EVAL\n",
        "    if not is_predicting:\n",
        "\n",
        "      (loss, predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      train_op = bert.optimization.create_optimizer(\n",
        "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
        "\n",
        "      # Calculate evaluation metrics. \n",
        "      def metric_fn(label_ids, predicted_labels):\n",
        "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
        "        f1_score = tf.contrib.metrics.f1_score(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        auc = tf.metrics.auc(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        recall = tf.metrics.recall(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        precision = tf.metrics.precision(\n",
        "            label_ids,\n",
        "            predicted_labels) \n",
        "        true_pos = tf.metrics.true_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        true_neg = tf.metrics.true_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)   \n",
        "        false_pos = tf.metrics.false_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)  \n",
        "        false_neg = tf.metrics.false_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"f1_score\": f1_score,\n",
        "            \"auc\": auc,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"true_positives\": true_pos,\n",
        "            \"true_negatives\": true_neg,\n",
        "            \"false_positives\": false_pos,\n",
        "            \"false_negatives\": false_neg\n",
        "        }\n",
        "\n",
        "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
        "\n",
        "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        return tf.estimator.EstimatorSpec(mode=mode,\n",
        "          loss=loss,\n",
        "          train_op=train_op)\n",
        "      else:\n",
        "          return tf.estimator.EstimatorSpec(mode=mode,\n",
        "            loss=loss,\n",
        "            eval_metric_ops=eval_metrics)\n",
        "    else:\n",
        "      (predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      predictions = {\n",
        "          'probabilities': log_probs,\n",
        "          'labels': predicted_labels\n",
        "      }\n",
        "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "\n",
        "  # Return the actual model function in the closure\n",
        "  return model_fn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OjwJ4bTeWXD8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Compute train and warmup steps from batch size\n",
        "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 3.0\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 500\n",
        "SAVE_SUMMARY_STEPS = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "emHf9GhfWBZ_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Compute # train and warmup steps from batch size\n",
        "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS) * 2\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oEJldMr3WYZa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Specify outpit directory and number of checkpoint steps to save\n",
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q_WebpS1X97v",
        "colab_type": "code",
        "outputId": "f836d663-6bed-48ad-9d20-e49af4e6243a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "cell_type": "code",
      "source": [
        "model_fn = model_fn_builder(\n",
        "  num_labels=len(label_list),\n",
        "  learning_rate=LEARNING_RATE,\n",
        "  num_train_steps=num_train_steps,\n",
        "  num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "estimator = tf.estimator.Estimator(\n",
        "  model_fn=model_fn,\n",
        "  config=run_config,\n",
        "  params={\"batch_size\": BATCH_SIZE})\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'OUTPUT_DIR_NAME', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8eb9e43f60>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NOO3RfG1DYLo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator. This is a pretty standard design pattern for working with Tensorflow [Estimators](https://www.tensorflow.org/guide/estimators)."
      ]
    },
    {
      "metadata": {
        "id": "1Pv2bAlOX_-K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create an input function for training. drop_remainder = True for using TPUs.\n",
        "train_input_fn = bert.run_classifier.input_fn_builder(\n",
        "    features=train_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=True,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t6Nukby2EB6-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we train our model! For me, using a Colab notebook running on Google's GPUs, my training time was about 14 minutes."
      ]
    },
    {
      "metadata": {
        "id": "nucD4gluYJmK",
        "colab_type": "code",
        "outputId": "4befecaf-7004-4d3f-e291-fb2449f92cfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1374
        }
      },
      "cell_type": "code",
      "source": [
        "print(f'Beginning Training!')\n",
        "current_time = datetime.now()\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print(\"Training took time \", datetime.now() - current_time)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beginning Training!\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/util.py:104: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "WARNING:tensorflow:From <ipython-input-17-ca03218f28a6>:34: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/learning_rate_decay_v2.py:321: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/metrics_impl.py:455: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into OUTPUT_DIR_NAME/model.ckpt.\n",
            "INFO:tensorflow:loss = 0.63074744, step = 0\n",
            "INFO:tensorflow:global_step/sec: 0.563682\n",
            "INFO:tensorflow:loss = 0.22760205, step = 101 (177.406 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.606754\n",
            "INFO:tensorflow:loss = 0.18947995, step = 200 (164.812 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.607164\n",
            "INFO:tensorflow:loss = 0.36446238, step = 300 (164.700 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.606738\n",
            "INFO:tensorflow:loss = 0.2703094, step = 400 (164.816 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 500 into OUTPUT_DIR_NAME/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.573131\n",
            "INFO:tensorflow:loss = 0.047156338, step = 500 (174.480 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.605676\n",
            "INFO:tensorflow:loss = 0.30006087, step = 600 (165.105 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.607518\n",
            "INFO:tensorflow:loss = 0.14915222, step = 700 (164.604 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.607291\n",
            "INFO:tensorflow:loss = 0.059713967, step = 800 (164.666 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.606492\n",
            "INFO:tensorflow:loss = 0.13314642, step = 900 (164.882 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1000 into OUTPUT_DIR_NAME/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.571697\n",
            "INFO:tensorflow:loss = 0.07390717, step = 1000 (174.918 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.606706\n",
            "INFO:tensorflow:loss = 0.005889723, step = 1100 (164.824 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.607749\n",
            "INFO:tensorflow:loss = 0.010327376, step = 1200 (164.542 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.606794\n",
            "INFO:tensorflow:loss = 0.00472295, step = 1300 (164.801 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.607456\n",
            "INFO:tensorflow:loss = 0.08506503, step = 1400 (164.621 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1500 into OUTPUT_DIR_NAME/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.572219\n",
            "INFO:tensorflow:loss = 0.0013601503, step = 1500 (174.758 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.608143\n",
            "INFO:tensorflow:loss = 0.001091466, step = 1600 (164.435 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.606083\n",
            "INFO:tensorflow:loss = 0.0009630093, step = 1700 (164.994 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.605039\n",
            "INFO:tensorflow:loss = 0.00078716705, step = 1800 (165.279 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1874 into OUTPUT_DIR_NAME/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.00075827754.\n",
            "Training took time  0:53:27.584257\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CmbLTVniARy3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's use our test data to see how well our model did:"
      ]
    },
    {
      "metadata": {
        "id": "JIhejfpyJ8Bx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_input_fn = run_classifier.input_fn_builder(\n",
        "    features=test_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PPVEXhNjYXC-",
        "colab_type": "code",
        "outputId": "42dde0bd-1a75-4779-d071-219982498fe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        }
      },
      "cell_type": "code",
      "source": [
        "estimator.evaluate(input_fn=test_input_fn, steps=None)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-02-20T22:24:15Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from OUTPUT_DIR_NAME/model.ckpt-1874\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2019-02-20-22:27:05\n",
            "INFO:tensorflow:Saving dict for global step 1874: auc = 0.59808373, eval_accuracy = 0.931, f1_score = 0.27368414, false_negatives = 459.0, false_positives = 231.0, global_step = 1874, loss = 0.39963886, precision = 0.3601108, recall = 0.22071308, true_negatives = 9180.0, true_positives = 130.0\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1874: OUTPUT_DIR_NAME/model.ckpt-1874\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'auc': 0.59808373,\n",
              " 'eval_accuracy': 0.931,\n",
              " 'f1_score': 0.27368414,\n",
              " 'false_negatives': 459.0,\n",
              " 'false_positives': 231.0,\n",
              " 'global_step': 1874,\n",
              " 'loss': 0.39963886,\n",
              " 'precision': 0.3601108,\n",
              " 'recall': 0.22071308,\n",
              " 'true_negatives': 9180.0,\n",
              " 'true_positives': 130.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "ueKsULteiz1B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's write code to make predictions on new sentences:"
      ]
    },
    {
      "metadata": {
        "id": "OsrbTD2EJTVl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def getPrediction(in_sentences):\n",
        "  labels = [\"Not an error\", \"Is an error\"]\n",
        "  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x[0], text_b = x[1], label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
        "  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
        "  predictions = estimator.predict(predict_input_fn)\n",
        "  return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-thbodgih_VJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred_sentences = [['In 2000 there war 11,1 milions boys and 21,6 milions girls, which is nearly similar to information about Africa in 2012. Next 12 years the quantity of boys dropped to 5,1 millions and girls – to 4,8 millions. The rest of world has changed its [MASK] of children without access to education twice, talking about girls, and less the twice thinking about boys. To sum up, from 2000 to 2012 there was trend of decreasing millions of boys and girls in all part of the world. The most successful changes are shown in South Asia.',\n",
        "  'amount',\n",
        "  1],\n",
        " ['Before thinking about how to solve the problem we should understand the reasons why it did appear. As it was stateted above, the reason of the increase in crime rates by young people is the exsessive control of them from parents and teachers. [MASK] latter in order to provide education, right behavior, and safety restrict a lot of things: from watching cruel TV-shows to visiting some places and communication with their friends. The reaction of young people is the protest against such norms. To show that they do not agree with the way parents and teachers bring them up, they do something prohibited.',\n",
        "  'The',\n",
        "  0],\n",
        " ['We are living in the liberty and democracy country, so not only artists but also everyone has truly rights to do what he or she wants. Creative artists, in particular, can feel respected when they have freedom to show their ideas to the world. They may feel [MASK] believe I them and always wait to see their new ideas such as words, pictures, music or film. In addition, this feeling can give them motivation which makes them create more wonderful production. Nevertheless, it will be too risk for the government to have no restriction on what artists do.',\n",
        "  'people',\n",
        "  0],\n",
        " ['It will be equal to less than 30%. In Japan number of elder calmly increases - there is no fast growth in 2030 like in the USA. In Sweden, growth [MASK] increases - the population of old people in 2040 will be near 25%. So, in all these countries the growth goes up and equal to 25%. Nowadays, the eldest country is Sweden.',\n",
        "  'fluently',\n",
        "  1]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QrZmvZySKQTm",
        "colab_type": "code",
        "outputId": "86ed668d-9bdf-4a17-874b-016afd38e47c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        }
      },
      "cell_type": "code",
      "source": [
        "predictions = getPrediction(pred_sentences)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 4\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] in 2000 there war 11 , 1 mil ##ions boys and 21 , 6 mil ##ions girls , which is nearly similar to information about africa in 2012 . next 12 years the quantity of boys dropped to 5 , 1 millions and girls – to 4 , 8 millions . the rest of world has changed its [MASK] of children without access to education twice , talking about girls , and less the twice thinking about boys . to sum up , from 2000 to 2012 there was trend of decreasing millions of boys and girls in all part of the world . the most successful changes are shown in south asia . [SEP] amount [SEP]\n",
            "INFO:tensorflow:input_ids: 101 1999 2456 2045 2162 2340 1010 1015 23689 8496 3337 1998 2538 1010 1020 23689 8496 3057 1010 2029 2003 3053 2714 2000 2592 2055 3088 1999 2262 1012 2279 2260 2086 1996 11712 1997 3337 3333 2000 1019 1010 1015 8817 1998 3057 1516 2000 1018 1010 1022 8817 1012 1996 2717 1997 2088 2038 2904 2049 103 1997 2336 2302 3229 2000 2495 3807 1010 3331 2055 3057 1010 1998 2625 1996 3807 3241 2055 3337 1012 2000 7680 2039 1010 2013 2456 2000 2262 2045 2001 9874 1997 16922 8817 1997 3337 1998 3057 1999 2035 2112 1997 1996 2088 1012 1996 2087 3144 3431 2024 3491 1999 2148 4021 1012 102 3815 102 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] before thinking about how to solve the problem we should understand the reasons why it did appear . as it was state ##ted above , the reason of the increase in crime rates by young people is the ex ##ses ##sive control of them from parents and teachers . [MASK] latter in order to provide education , right behavior , and safety restrict a lot of things : from watching cruel tv - shows to visiting some places and communication with their friends . the reaction of young people is the protest against such norms . to show that they do not agree with the way parents and teachers bring them up , they do something prohibited . [SEP] the [SEP]\n",
            "INFO:tensorflow:input_ids: 101 2077 3241 2055 2129 2000 9611 1996 3291 2057 2323 3305 1996 4436 2339 2009 2106 3711 1012 2004 2009 2001 2110 3064 2682 1010 1996 3114 1997 1996 3623 1999 4126 6165 2011 2402 2111 2003 1996 4654 8583 12742 2491 1997 2068 2013 3008 1998 5089 1012 103 3732 1999 2344 2000 3073 2495 1010 2157 5248 1010 1998 3808 21573 1037 2843 1997 2477 1024 2013 3666 10311 2694 1011 3065 2000 5873 2070 3182 1998 4807 2007 2037 2814 1012 1996 4668 1997 2402 2111 2003 1996 6186 2114 2107 17606 1012 2000 2265 2008 2027 2079 2025 5993 2007 1996 2126 3008 1998 5089 3288 2068 2039 1010 2027 2079 2242 10890 1012 102 1996 102 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] we are living in the liberty and democracy country , so not only artists but also everyone has truly rights to do what he or she wants . creative artists , in particular , can feel respected when they have freedom to show their ideas to the world . they may feel [MASK] believe i them and always wait to see their new ideas such as words , pictures , music or film . in addition , this feeling can give them motivation which makes them create more wonderful production . nevertheless , it will be too risk for the government to have no restriction on what artists do . [SEP] people [SEP]\n",
            "INFO:tensorflow:input_ids: 101 2057 2024 2542 1999 1996 7044 1998 7072 2406 1010 2061 2025 2069 3324 2021 2036 3071 2038 5621 2916 2000 2079 2054 2002 2030 2016 4122 1012 5541 3324 1010 1999 3327 1010 2064 2514 9768 2043 2027 2031 4071 2000 2265 2037 4784 2000 1996 2088 1012 2027 2089 2514 103 2903 1045 2068 1998 2467 3524 2000 2156 2037 2047 4784 2107 2004 2616 1010 4620 1010 2189 2030 2143 1012 1999 2804 1010 2023 3110 2064 2507 2068 14354 2029 3084 2068 3443 2062 6919 2537 1012 6600 1010 2009 2097 2022 2205 3891 2005 1996 2231 2000 2031 2053 16840 2006 2054 3324 2079 1012 102 2111 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] it will be equal to less than 30 % . in japan number of elder calmly increases - there is no fast growth in 203 ##0 like in the usa . in sweden , growth [MASK] increases - the population of old people in 204 ##0 will be near 25 % . so , in all these countries the growth goes up and equal to 25 % . nowadays , the eldest country is sweden . [SEP] fluent ##ly [SEP]\n",
            "INFO:tensorflow:input_ids: 101 2009 2097 2022 5020 2000 2625 2084 2382 1003 1012 1999 2900 2193 1997 6422 12885 7457 1011 2045 2003 2053 3435 3930 1999 18540 2692 2066 1999 1996 3915 1012 1999 4701 1010 3930 103 7457 1011 1996 2313 1997 2214 2111 1999 19627 2692 2097 2022 2379 2423 1003 1012 2061 1010 1999 2035 2122 3032 1996 3930 3632 2039 1998 5020 2000 2423 1003 1012 13367 1010 1996 7310 2406 2003 4701 1012 102 19376 2135 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from OUTPUT_DIR_NAME/model.ckpt-750\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MXkRiEBUqN3n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Voila! We have a sentiment classifier!"
      ]
    },
    {
      "metadata": {
        "id": "ERkTE8-7oQLZ",
        "colab_type": "code",
        "outputId": "55a016b4-5dc0-46a6-aa78-a9d6ce6a53a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        }
      },
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['In 2000 there war 11,1 milions boys and 21,6 milions girls, which is nearly similar to information about Africa in 2012. Next 12 years the quantity of boys dropped to 5,1 millions and girls – to 4,8 millions. The rest of world has changed its [MASK] of children without access to education twice, talking about girls, and less the twice thinking about boys. To sum up, from 2000 to 2012 there was trend of decreasing millions of boys and girls in all part of the world. The most successful changes are shown in South Asia.',\n",
              "   'amount',\n",
              "   1],\n",
              "  array([-1.7102101 , -0.19946091], dtype=float32),\n",
              "  'Is an error'),\n",
              " (['Before thinking about how to solve the problem we should understand the reasons why it did appear. As it was stateted above, the reason of the increase in crime rates by young people is the exsessive control of them from parents and teachers. [MASK] latter in order to provide education, right behavior, and safety restrict a lot of things: from watching cruel TV-shows to visiting some places and communication with their friends. The reaction of young people is the protest against such norms. To show that they do not agree with the way parents and teachers bring them up, they do something prohibited.',\n",
              "   'The',\n",
              "   0],\n",
              "  array([-1.8735252e-03, -6.2808580e+00], dtype=float32),\n",
              "  'Not an error'),\n",
              " (['We are living in the liberty and democracy country, so not only artists but also everyone has truly rights to do what he or she wants. Creative artists, in particular, can feel respected when they have freedom to show their ideas to the world. They may feel [MASK] believe I them and always wait to see their new ideas such as words, pictures, music or film. In addition, this feeling can give them motivation which makes them create more wonderful production. Nevertheless, it will be too risk for the government to have no restriction on what artists do.',\n",
              "   'people',\n",
              "   0],\n",
              "  array([-4.1219755e-03, -5.4934778e+00], dtype=float32),\n",
              "  'Not an error'),\n",
              " (['It will be equal to less than 30%. In Japan number of elder calmly increases - there is no fast growth in 2030 like in the USA. In Sweden, growth [MASK] increases - the population of old people in 2040 will be near 25%. So, in all these countries the growth goes up and equal to 25%. Nowadays, the eldest country is Sweden.',\n",
              "   'fluently',\n",
              "   1],\n",
              "  array([-3.244938  , -0.03975067], dtype=float32),\n",
              "  'Is an error')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "metadata": {
        "id": "uettEz32yqIN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Extracting the trained model\n",
        "Using the following procedures we can extract the resulting state of now trained model.\n"
      ]
    },
    {
      "metadata": {
        "id": "eA1IGvdxw0UC",
        "colab_type": "code",
        "outputId": "11f5f3c3-38cd-49be-dc12-17e9d7195cf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "cell_type": "code",
      "source": [
        "os.listdir('.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'OUTPUT_DIR_NAME',\n",
              " 'wsites.json',\n",
              " 'wsites_bert15x1-take1.json',\n",
              " 'adc.json',\n",
              " 'wsites_bert1x1-take1-750steps.json',\n",
              " 'testigo.html',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "ypwldvtkxELV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once in a notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2G-laplexKFD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hD1vahs6xeCT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('OUTPUT_DIR_NAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8JA1r_14xlhz",
        "colab_type": "code",
        "outputId": "deda7a13-5f13-4e99-f98d-4934218990be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "cell_type": "code",
      "source": [
        "os.listdir('.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['model.ckpt-500.index',\n",
              " 'model.ckpt-500.meta',\n",
              " 'eval',\n",
              " 'model.ckpt-1000.meta',\n",
              " 'model.ckpt-1500.meta',\n",
              " 'checkpoint',\n",
              " 'model.ckpt-1500.index',\n",
              " 'model.ckpt-1874.data-00000-of-00001',\n",
              " 'events.out.tfevents.1550698238.909fd5543523',\n",
              " 'model.ckpt-500.data-00000-of-00001',\n",
              " 'model.ckpt-1874.index',\n",
              " 'model.ckpt-1000.index',\n",
              " 'model.ckpt-0.data-00000-of-00001',\n",
              " 'graph.pbtxt',\n",
              " 'model.ckpt-1000.data-00000-of-00001',\n",
              " 'model.ckpt-0.index',\n",
              " 'model.ckpt-1874.meta',\n",
              " 'model.ckpt-0.meta',\n",
              " 'model.ckpt-1500.data-00000-of-00001']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "AhFLF6XGxrD1",
        "colab_type": "code",
        "outputId": "b8d57655-0606-4343-8190-48721b2ad17f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Create & upload a file.\n",
        "uploaded = drive.CreateFile({'title': 'model.ckpt-1874.index'})\n",
        "uploaded.SetContentFile('model.ckpt-1874.index')\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 1xlwANS_Kc3hwdz-_56eMFcIGlCOkZ02Q\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B2B28k13yYGs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('..')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ess9ix58ybyL",
        "colab_type": "code",
        "outputId": "3421cd76-1b26-4b04-e572-974f03ac32bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "cell_type": "code",
      "source": [
        "os.listdir('.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'OUTPUT_DIR_NAME',\n",
              " 'wsites.json',\n",
              " 'wsites_bert15x1-take1.json',\n",
              " 'adc.json',\n",
              " 'wsites_bert1x1-take1-750steps.json',\n",
              " 'testigo.html',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "Zk7zf0Qu3Bp8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Preparing existing model for REALEC production"
      ]
    },
    {
      "metadata": {
        "id": "4CbCTzuk3SL9",
        "colab_type": "code",
        "outputId": "a1a9ae96-16cd-4d96-ede0-dedeadcc0bc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import itertools\n",
        "\n",
        "nltk.download('perluniprops')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
            "[nltk_data]   Package perluniprops is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "iPJwPeOa80Qc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tknzr = TweetTokenizer()\n",
        "\n",
        "from nltk.tokenize.moses import MosesDetokenizer\n",
        "detokenizer = MosesDetokenizer()\n",
        "\n",
        "def annotate(text):\n",
        "  current_time = datetime.now()\n",
        "  raw = text\n",
        "  raw = re.sub(r'\\s', ' ', raw)\n",
        "  raw = re.sub(r'( )+', ' ', raw)\n",
        "  sentences = nltk.sent_tokenize(raw)\n",
        "  wordsoup = []\n",
        "  annotation_set = []\n",
        "  checking_ids = []\n",
        "  tokens = []\n",
        "  J = 0\n",
        "  for sentence in sentences:\n",
        "    wordsoup.append(tknzr.tokenize(sentence))\n",
        "  for i in range(len(wordsoup)):\n",
        "    tokenized_sentence = wordsoup[i]\n",
        "    for j in range(len(tokenized_sentence)):\n",
        "      token = tokenized_sentence[j]\n",
        "      if not re.search(r'[a-zA-Z]', token):\n",
        "        tokens.append([token, 0])\n",
        "      else:\n",
        "        tokens.append([token, None])\n",
        "        substring = token\n",
        "        si = i-2\n",
        "        if si < 0:\n",
        "          si = 0\n",
        "        detokenizing_string = wordsoup[si:i] + [tokenized_sentence[:j] + ['[MASK]'] + tokenized_sentence[j+1:]] + wordsoup[i+1:i+3]\n",
        "        detokenizing_string = [item for sublist in detokenizing_string for item in sublist]\n",
        "        entry = detokenizer.detokenize(detokenizing_string, return_str=True)\n",
        "        annotation_set.append([entry, substring])\n",
        "        checking_ids.append(J)\n",
        "      J += 1\n",
        "  predictions = getPrediction(annotation_set)\n",
        "  for p in range(len(predictions)):\n",
        "    if predictions[p][-1] == 'Is an error':\n",
        "      tokens[checking_ids[p]][1] = 1\n",
        "    else:\n",
        "      tokens[checking_ids[p]][1] = 0\n",
        "  print('\\n')\n",
        "  print(\"Annotation took time \", datetime.now() - current_time)\n",
        "  return tokens\n",
        "\n",
        "def print_annotated_webpage(tokens, title):\n",
        "  out = '<html>\\n<head>\\n<title>'+title+'</title>\\n<meta charset=\"utf-8\">\\n<style type=\"text/css\">\\n.blue {\\n\\tbackground: #a8d1ff;\\n\\tdisplay: inline-block;\\n}\\n</style>\\n</head>\\n<body>'\n",
        "  outsoup = []\n",
        "  for token in tokens:\n",
        "    if token[1] == 1:\n",
        "      token[0] = '<div class=\"blue\">'+token[0]+'</div>'\n",
        "    outsoup.append(token[0])\n",
        "  bodystring = detokenizer.detokenize(outsoup, return_str=True)\n",
        "  out += bodystring\n",
        "  out += '</body>\\n</html>'\n",
        "  return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XlKufHUaAim3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Model_name = 'bert15x1-take2-1874steps'  #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lrnyWIYaCNng",
        "colab_type": "code",
        "outputId": "2a4f3799-82a7-492f-9320-576077dde269",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4085
        }
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "curname = \"wsites_\" + Model_name + \".json\"\n",
        "\n",
        "test_texts = {'DTi_50_2': \"\"\"It is widely known that music labels and film makers lose a great deal of money every year from illegal copying and free internet sharing. Some people say that people who do that should be punished, some think that such men are the new «Robin Hoods». Let us take a look at this problem.\n",
        "On the one hand, illegal copying is prohibited in almost all civilized countries and there is a reason for that. Music, books and films are considered as an intellectual property and it is quite uderstandable because artists are getting paid for composing, painting and film making only if their products sell, and if they do not have enough money for their living and creating they will just get another job, and this is why the law of intellectual property exists. It helps artists to get money they deserve and to have enough funds to make a quality product.\n",
        "On the other hand, nowadays it is not so simple as it may look like at first. Musicians do not often need labels to record their masterpieces anymore because personal computers went so far that now you can record some high-quality sound right in your living room so you do not need to rent a studio for that. As for film makers, they get millions just by dressing the main character in a big logo T-shirt of some reach corporation. In fact, money, that a film company is paid for commercial, can sometimes fully cover the film making expences. \n",
        "To conclude, I would like to say that we should obey the law of intellectual property. If we want to live in a respectful society, but sometimes, I think we should also ask ourselves what we are paying for.\"\"\",\n",
        "'VSa_105_1': \"\"\"The graph provides information about development of the book sales system in 4 different countries (USA, Germany, China, UK) in 2014 and predicts future perspectives of this industry in 2018.\n",
        "Generally speaking, it is observed a great increase of eBook on the market in the USA in 2018, which will surpass printed sources. By contrast, in other countries, except the UK, printed literature will remain constant position.\n",
        "Despite the fact that print dominated on the book market of the USA in 2014 and exceeded eBook by a half (10,5 to 5,5 respectively), in 2018 it is forecasted that benefit of print literature will shrink on 3 billion US dollars. At the same time the income of eBook will grow on 3 billion US dollars and will be the most selling source of literature (8,5 billion $ of Ebook as against 7,5 billion $ of print).\n",
        "In other countries the book market is not so developed as it in the USA. However, it is forecasted that in 2018 the ebook sales will slightly overtake printed book sales (2,3 to 2 respectively).\n",
        "In Germany and China printed books will be the most popular kind of literature (6 to 4,2 respectively), especially in Germany print indicators remain stable.\n",
        "But it also be a small growth in sales of Ebooks in these countries (from 1 to 1,5 in German and from 0,5 to 1 in China).\"\"\",\n",
        "'NMya_90_1': \"\"\"On the first graph we can see information about maximal and minimal average temperatures in Yakutsk. Graphs for both, the minimum and maximum have a parabolic appearance: they both have a steady increase until the hottest month of the year (july) and after that they both have as constant decrease down to december. Analazing this whole graph, you can point out several things. First of all, the difference of the highest and lowest temperatures between the coldest month and the hottest month is sixty degrees and fifty degrees respectively. The biggest difference between maximum and minimum temperatures is seen on march and it is about seventeen degrees. According to this graph, july is always the hottest month and january is the coldest.\n",
        "\n",
        "On the second graph we can see similar temperature comparsions for Rio de Janeiro. Two lines represent minimum and maximum average temperatures for every month of the year. According to this graph, temperatures in Rio don’t change much throughout the year. the maximal difference of minimum and maximum temperature is on january, july and august and is approximately only seven degrees. For this city the coldest months are june and july, and the hottest are january and february.\n",
        "\n",
        "Comparing the two graphs we notice this: average year temperature in rio is almost constant, comparing to Yakutsk and is never less then 18 °C, and the coldest and the hottest months in Rio are the hottest and the coldest in Yakutsk.\n",
        "\"\"\",\n",
        "'NChe_16_2': \"\"\"In reacent time a lot of people claims that modern technology is a curse that leads to a health problems. It is no use to argue with this statement.\n",
        "For begining,  many doctors already said, that such things like computers can caus a lot of prolems with heath. So, one of the most famous illness is damaged eyes. Then kids or adults spend a load of time next to monitor they starting to lose eyes sharphes. What is more, some scientiest claiws that some deuices cau lead euen cancer. There is no uniqe poiut betwine doctors, any way there is such kind of danger.\n",
        "As for my, I find this problems not so unreducable. In general, all health desiases caused by modern techuologies were caused by ouer-use of this technologies and useing it in anpropriate way. So, if people start to control time that they spend with there deuices it will help to recduce a lot of problems with eyes. As a next step, we should understand that lightning aroun us is also very important. That means, that modern technologies haue some bad influens on people, but we can also dicrease that bad effect.\n",
        "To sum up, wiede use of modern technology shown us that there are not only benefits in deuelopmeut of deuices, but also the great danger. But corect use of it can reduce to the minimum all harmful effects\"\"\",\n",
        "'ESha_3_2': \"\"\"Nowadays it is becoming cosier to express yourself by different ways. Some people do it by using words, some by using pictures of films. But is it clear to allow artist to do and to act how they want? \n",
        "\n",
        "\n",
        "There are so many ways to express your thoghts and feelings. People from ancient times show their ideas and thoughts by paintings and music. A lot of paintings of famous authors are held now in different galleries and big amount of people see them every day. But in long time ago just really talanted people become famous and well known artists.\n",
        "\n",
        "\n",
        "Now situation is different. Every person can become an artist. Sometimes, people don't think how their ideas and works would influence other people's minds. Too many untalanted and unproffesional people create music, movies and paintings. It is really hard for a good artist to show himself in such amount of untalanted people.\n",
        "\n",
        "\n",
        "In my opinion, government should allowed to express ideas and thoughts stronly really good artists. I their ideas are clear and they have something to show and tell people, they should do it. But if artist's works don't have any idea or logic or it can't bring anything good to public, there is nothing to show.\n",
        "\n",
        "\n",
        "Every person have a chance to show itself by any way he choose. But at first, we should think, if he really wants it and if she really have something to say and show to bublicity. Only really good works should be shown to people, because it can influence them and their minds a lot.\n",
        "\"\"\"}\n",
        "\n",
        "outie = [print_annotated_webpage(annotate(test_texts[tt]), tt) for tt in test_texts]\n",
        "with open(curname, 'w', encoding=\"utf-8\") as w:\n",
        "  json.dump(outie, w)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 290\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] [MASK] is widely known that music labels and film makers lose a great deal of money every year from illegal copying and free internet sharing . some people say that people who do that should be punished , some think that such men are the new « robin hood ##s » . let us take a look at this problem . [SEP] it [SEP]\n",
            "INFO:tensorflow:input_ids: 101 103 2003 4235 2124 2008 2189 10873 1998 2143 11153 4558 1037 2307 3066 1997 2769 2296 2095 2013 6206 24731 1998 2489 4274 6631 1012 2070 2111 2360 2008 2111 2040 2079 2008 2323 2022 14248 1010 2070 2228 2008 2107 2273 2024 1996 2047 1077 5863 7415 2015 1090 1012 2292 2149 2202 1037 2298 2012 2023 3291 1012 102 2009 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] it [MASK] widely known that music labels and film makers lose a great deal of money every year from illegal copying and free internet sharing . some people say that people who do that should be punished , some think that such men are the new « robin hood ##s » . let us take a look at this problem . [SEP] is [SEP]\n",
            "INFO:tensorflow:input_ids: 101 2009 103 4235 2124 2008 2189 10873 1998 2143 11153 4558 1037 2307 3066 1997 2769 2296 2095 2013 6206 24731 1998 2489 4274 6631 1012 2070 2111 2360 2008 2111 2040 2079 2008 2323 2022 14248 1010 2070 2228 2008 2107 2273 2024 1996 2047 1077 5863 7415 2015 1090 1012 2292 2149 2202 1037 2298 2012 2023 3291 1012 102 2003 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] it is [MASK] known that music labels and film makers lose a great deal of money every year from illegal copying and free internet sharing . some people say that people who do that should be punished , some think that such men are the new « robin hood ##s » . let us take a look at this problem . [SEP] widely [SEP]\n",
            "INFO:tensorflow:input_ids: 101 2009 2003 103 2124 2008 2189 10873 1998 2143 11153 4558 1037 2307 3066 1997 2769 2296 2095 2013 6206 24731 1998 2489 4274 6631 1012 2070 2111 2360 2008 2111 2040 2079 2008 2323 2022 14248 1010 2070 2228 2008 2107 2273 2024 1996 2047 1077 5863 7415 2015 1090 1012 2292 2149 2202 1037 2298 2012 2023 3291 1012 102 4235 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] it is widely [MASK] that music labels and film makers lose a great deal of money every year from illegal copying and free internet sharing . some people say that people who do that should be punished , some think that such men are the new « robin hood ##s » . let us take a look at this problem . [SEP] known [SEP]\n",
            "INFO:tensorflow:input_ids: 101 2009 2003 4235 103 2008 2189 10873 1998 2143 11153 4558 1037 2307 3066 1997 2769 2296 2095 2013 6206 24731 1998 2489 4274 6631 1012 2070 2111 2360 2008 2111 2040 2079 2008 2323 2022 14248 1010 2070 2228 2008 2107 2273 2024 1996 2047 1077 5863 7415 2015 1090 1012 2292 2149 2202 1037 2298 2012 2023 3291 1012 102 2124 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] it is widely known [MASK] music labels and film makers lose a great deal of money every year from illegal copying and free internet sharing . some people say that people who do that should be punished , some think that such men are the new « robin hood ##s » . let us take a look at this problem . [SEP] that [SEP]\n",
            "INFO:tensorflow:input_ids: 101 2009 2003 4235 2124 103 2189 10873 1998 2143 11153 4558 1037 2307 3066 1997 2769 2296 2095 2013 6206 24731 1998 2489 4274 6631 1012 2070 2111 2360 2008 2111 2040 2079 2008 2323 2022 14248 1010 2070 2228 2008 2107 2273 2024 1996 2047 1077 5863 7415 2015 1090 1012 2292 2149 2202 1037 2298 2012 2023 3291 1012 102 2008 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from OUTPUT_DIR_NAME/model.ckpt-1874\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "\n",
            "\n",
            "Annotation took time  0:00:13.455726\n",
            "INFO:tensorflow:Writing example 0 of 209\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] [MASK] graph provides information about development of the book sales system in 4 different countries ( usa , germany , china , uk ) in 2014 and predict ##s future perspectives of this industry in 2018 . generally speaking , it is observed a great increase of ebook on the market in the usa in 2018 , which will sur ##pass printed sources . by contrast , in other countries , except the uk , printed literature will remain constant position . [SEP] the [SEP]\n",
            "INFO:tensorflow:input_ids: 101 103 10629 3640 2592 2055 2458 1997 1996 2338 4341 2291 1999 1018 2367 3032 1006 3915 1010 2762 1010 2859 1010 2866 1007 1999 2297 1998 16014 2015 2925 15251 1997 2023 3068 1999 2760 1012 3227 4092 1010 2009 2003 5159 1037 2307 3623 1997 26885 2006 1996 3006 1999 1996 3915 1999 2760 1010 2029 2097 7505 15194 6267 4216 1012 2011 5688 1010 1999 2060 3032 1010 3272 1996 2866 1010 6267 3906 2097 3961 5377 2597 1012 102 1996 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] the [MASK] provides information about development of the book sales system in 4 different countries ( usa , germany , china , uk ) in 2014 and predict ##s future perspectives of this industry in 2018 . generally speaking , it is observed a great increase of ebook on the market in the usa in 2018 , which will sur ##pass printed sources . by contrast , in other countries , except the uk , printed literature will remain constant position . [SEP] graph [SEP]\n",
            "INFO:tensorflow:input_ids: 101 1996 103 3640 2592 2055 2458 1997 1996 2338 4341 2291 1999 1018 2367 3032 1006 3915 1010 2762 1010 2859 1010 2866 1007 1999 2297 1998 16014 2015 2925 15251 1997 2023 3068 1999 2760 1012 3227 4092 1010 2009 2003 5159 1037 2307 3623 1997 26885 2006 1996 3006 1999 1996 3915 1999 2760 1010 2029 2097 7505 15194 6267 4216 1012 2011 5688 1010 1999 2060 3032 1010 3272 1996 2866 1010 6267 3906 2097 3961 5377 2597 1012 102 10629 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] the graph [MASK] information about development of the book sales system in 4 different countries ( usa , germany , china , uk ) in 2014 and predict ##s future perspectives of this industry in 2018 . generally speaking , it is observed a great increase of ebook on the market in the usa in 2018 , which will sur ##pass printed sources . by contrast , in other countries , except the uk , printed literature will remain constant position . [SEP] provides [SEP]\n",
            "INFO:tensorflow:input_ids: 101 1996 10629 103 2592 2055 2458 1997 1996 2338 4341 2291 1999 1018 2367 3032 1006 3915 1010 2762 1010 2859 1010 2866 1007 1999 2297 1998 16014 2015 2925 15251 1997 2023 3068 1999 2760 1012 3227 4092 1010 2009 2003 5159 1037 2307 3623 1997 26885 2006 1996 3006 1999 1996 3915 1999 2760 1010 2029 2097 7505 15194 6267 4216 1012 2011 5688 1010 1999 2060 3032 1010 3272 1996 2866 1010 6267 3906 2097 3961 5377 2597 1012 102 3640 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] the graph provides [MASK] about development of the book sales system in 4 different countries ( usa , germany , china , uk ) in 2014 and predict ##s future perspectives of this industry in 2018 . generally speaking , it is observed a great increase of ebook on the market in the usa in 2018 , which will sur ##pass printed sources . by contrast , in other countries , except the uk , printed literature will remain constant position . [SEP] information [SEP]\n",
            "INFO:tensorflow:input_ids: 101 1996 10629 3640 103 2055 2458 1997 1996 2338 4341 2291 1999 1018 2367 3032 1006 3915 1010 2762 1010 2859 1010 2866 1007 1999 2297 1998 16014 2015 2925 15251 1997 2023 3068 1999 2760 1012 3227 4092 1010 2009 2003 5159 1037 2307 3623 1997 26885 2006 1996 3006 1999 1996 3915 1999 2760 1010 2029 2097 7505 15194 6267 4216 1012 2011 5688 1010 1999 2060 3032 1010 3272 1996 2866 1010 6267 3906 2097 3961 5377 2597 1012 102 2592 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] the graph provides information [MASK] development of the book sales system in 4 different countries ( usa , germany , china , uk ) in 2014 and predict ##s future perspectives of this industry in 2018 . generally speaking , it is observed a great increase of ebook on the market in the usa in 2018 , which will sur ##pass printed sources . by contrast , in other countries , except the uk , printed literature will remain constant position . [SEP] about [SEP]\n",
            "INFO:tensorflow:input_ids: 101 1996 10629 3640 2592 103 2458 1997 1996 2338 4341 2291 1999 1018 2367 3032 1006 3915 1010 2762 1010 2859 1010 2866 1007 1999 2297 1998 16014 2015 2925 15251 1997 2023 3068 1999 2760 1012 3227 4092 1010 2009 2003 5159 1037 2307 3623 1997 26885 2006 1996 3006 1999 1996 3915 1999 2760 1010 2029 2097 7505 15194 6267 4216 1012 2011 5688 1010 1999 2060 3032 1010 3272 1996 2866 1010 6267 3906 2097 3961 5377 2597 1012 102 2055 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from OUTPUT_DIR_NAME/model.ckpt-1874\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "\n",
            "\n",
            "Annotation took time  0:00:11.745770\n",
            "INFO:tensorflow:Writing example 0 of 242\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] [MASK] the first graph we can see information about maximal and minimal average temperatures in ya ##ku ##tsk . graphs for both , the minimum and maximum have a para ##bolic appearance : they both have a steady increase until the hottest month of the year ( july ) and after that they both have as constant decrease down to december . anal ##azi ##ng this whole graph , you can point out several things . [SEP] on [SEP]\n",
            "INFO:tensorflow:input_ids: 101 103 1996 2034 10629 2057 2064 2156 2592 2055 29160 1998 10124 2779 7715 1999 8038 5283 29064 1012 19287 2005 2119 1010 1996 6263 1998 4555 2031 1037 11498 18647 3311 1024 2027 2119 2031 1037 6706 3623 2127 1996 20930 3204 1997 1996 2095 1006 2251 1007 1998 2044 2008 2027 2119 2031 2004 5377 9885 2091 2000 2285 1012 20302 16103 3070 2023 2878 10629 1010 2017 2064 2391 2041 2195 2477 1012 102 2006 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] on [MASK] first graph we can see information about maximal and minimal average temperatures in ya ##ku ##tsk . graphs for both , the minimum and maximum have a para ##bolic appearance : they both have a steady increase until the hottest month of the year ( july ) and after that they both have as constant decrease down to december . anal ##azi ##ng this whole graph , you can point out several things . [SEP] the [SEP]\n",
            "INFO:tensorflow:input_ids: 101 2006 103 2034 10629 2057 2064 2156 2592 2055 29160 1998 10124 2779 7715 1999 8038 5283 29064 1012 19287 2005 2119 1010 1996 6263 1998 4555 2031 1037 11498 18647 3311 1024 2027 2119 2031 1037 6706 3623 2127 1996 20930 3204 1997 1996 2095 1006 2251 1007 1998 2044 2008 2027 2119 2031 2004 5377 9885 2091 2000 2285 1012 20302 16103 3070 2023 2878 10629 1010 2017 2064 2391 2041 2195 2477 1012 102 1996 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] on the [MASK] graph we can see information about maximal and minimal average temperatures in ya ##ku ##tsk . graphs for both , the minimum and maximum have a para ##bolic appearance : they both have a steady increase until the hottest month of the year ( july ) and after that they both have as constant decrease down to december . anal ##azi ##ng this whole graph , you can point out several things . [SEP] first [SEP]\n",
            "INFO:tensorflow:input_ids: 101 2006 1996 103 10629 2057 2064 2156 2592 2055 29160 1998 10124 2779 7715 1999 8038 5283 29064 1012 19287 2005 2119 1010 1996 6263 1998 4555 2031 1037 11498 18647 3311 1024 2027 2119 2031 1037 6706 3623 2127 1996 20930 3204 1997 1996 2095 1006 2251 1007 1998 2044 2008 2027 2119 2031 2004 5377 9885 2091 2000 2285 1012 20302 16103 3070 2023 2878 10629 1010 2017 2064 2391 2041 2195 2477 1012 102 2034 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] on the first [MASK] we can see information about maximal and minimal average temperatures in ya ##ku ##tsk . graphs for both , the minimum and maximum have a para ##bolic appearance : they both have a steady increase until the hottest month of the year ( july ) and after that they both have as constant decrease down to december . anal ##azi ##ng this whole graph , you can point out several things . [SEP] graph [SEP]\n",
            "INFO:tensorflow:input_ids: 101 2006 1996 2034 103 2057 2064 2156 2592 2055 29160 1998 10124 2779 7715 1999 8038 5283 29064 1012 19287 2005 2119 1010 1996 6263 1998 4555 2031 1037 11498 18647 3311 1024 2027 2119 2031 1037 6706 3623 2127 1996 20930 3204 1997 1996 2095 1006 2251 1007 1998 2044 2008 2027 2119 2031 2004 5377 9885 2091 2000 2285 1012 20302 16103 3070 2023 2878 10629 1010 2017 2064 2391 2041 2195 2477 1012 102 10629 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] on the first graph [MASK] can see information about maximal and minimal average temperatures in ya ##ku ##tsk . graphs for both , the minimum and maximum have a para ##bolic appearance : they both have a steady increase until the hottest month of the year ( july ) and after that they both have as constant decrease down to december . anal ##azi ##ng this whole graph , you can point out several things . [SEP] we [SEP]\n",
            "INFO:tensorflow:input_ids: 101 2006 1996 2034 10629 103 2064 2156 2592 2055 29160 1998 10124 2779 7715 1999 8038 5283 29064 1012 19287 2005 2119 1010 1996 6263 1998 4555 2031 1037 11498 18647 3311 1024 2027 2119 2031 1037 6706 3623 2127 1996 20930 3204 1997 1996 2095 1006 2251 1007 1998 2044 2008 2027 2119 2031 2004 5377 9885 2091 2000 2285 1012 20302 16103 3070 2023 2878 10629 1010 2017 2064 2391 2041 2195 2477 1012 102 2057 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from OUTPUT_DIR_NAME/model.ckpt-1874\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "\n",
            "\n",
            "Annotation took time  0:00:12.568108\n",
            "INFO:tensorflow:Writing example 0 of 232\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] [MASK] re ##ace ##nt time a lot of people claims that modern technology is a curse that leads to a health problems . it is no use to argue with this statement . for begin ##ing , many doctors already said , that such things like computers can ca ##us a lot of pro ##lem ##s with heath . [SEP] in [SEP]\n",
            "INFO:tensorflow:input_ids: 101 103 2128 10732 3372 2051 1037 2843 1997 2111 4447 2008 2715 2974 2003 1037 8364 2008 5260 2000 1037 2740 3471 1012 2009 2003 2053 2224 2000 7475 2007 2023 4861 1012 2005 4088 2075 1010 2116 7435 2525 2056 1010 2008 2107 2477 2066 7588 2064 6187 2271 1037 2843 1997 4013 16930 2015 2007 9895 1012 102 1999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] in [MASK] time a lot of people claims that modern technology is a curse that leads to a health problems . it is no use to argue with this statement . for begin ##ing , many doctors already said , that such things like computers can ca ##us a lot of pro ##lem ##s with heath . [SEP] re ##ace ##nt [SEP]\n",
            "INFO:tensorflow:input_ids: 101 1999 103 2051 1037 2843 1997 2111 4447 2008 2715 2974 2003 1037 8364 2008 5260 2000 1037 2740 3471 1012 2009 2003 2053 2224 2000 7475 2007 2023 4861 1012 2005 4088 2075 1010 2116 7435 2525 2056 1010 2008 2107 2477 2066 7588 2064 6187 2271 1037 2843 1997 4013 16930 2015 2007 9895 1012 102 2128 10732 3372 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] in re ##ace ##nt [MASK] a lot of people claims that modern technology is a curse that leads to a health problems . it is no use to argue with this statement . for begin ##ing , many doctors already said , that such things like computers can ca ##us a lot of pro ##lem ##s with heath . [SEP] time [SEP]\n",
            "INFO:tensorflow:input_ids: 101 1999 2128 10732 3372 103 1037 2843 1997 2111 4447 2008 2715 2974 2003 1037 8364 2008 5260 2000 1037 2740 3471 1012 2009 2003 2053 2224 2000 7475 2007 2023 4861 1012 2005 4088 2075 1010 2116 7435 2525 2056 1010 2008 2107 2477 2066 7588 2064 6187 2271 1037 2843 1997 4013 16930 2015 2007 9895 1012 102 2051 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] in re ##ace ##nt time [MASK] lot of people claims that modern technology is a curse that leads to a health problems . it is no use to argue with this statement . for begin ##ing , many doctors already said , that such things like computers can ca ##us a lot of pro ##lem ##s with heath . [SEP] a [SEP]\n",
            "INFO:tensorflow:input_ids: 101 1999 2128 10732 3372 2051 103 2843 1997 2111 4447 2008 2715 2974 2003 1037 8364 2008 5260 2000 1037 2740 3471 1012 2009 2003 2053 2224 2000 7475 2007 2023 4861 1012 2005 4088 2075 1010 2116 7435 2525 2056 1010 2008 2107 2477 2066 7588 2064 6187 2271 1037 2843 1997 4013 16930 2015 2007 9895 1012 102 1037 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] in re ##ace ##nt time a [MASK] of people claims that modern technology is a curse that leads to a health problems . it is no use to argue with this statement . for begin ##ing , many doctors already said , that such things like computers can ca ##us a lot of pro ##lem ##s with heath . [SEP] lot [SEP]\n",
            "INFO:tensorflow:input_ids: 101 1999 2128 10732 3372 2051 1037 103 1997 2111 4447 2008 2715 2974 2003 1037 8364 2008 5260 2000 1037 2740 3471 1012 2009 2003 2053 2224 2000 7475 2007 2023 4861 1012 2005 4088 2075 1010 2116 7435 2525 2056 1010 2008 2107 2477 2066 7588 2064 6187 2271 1037 2843 1997 4013 16930 2015 2007 9895 1012 102 2843 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from OUTPUT_DIR_NAME/model.ckpt-1874\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "\n",
            "\n",
            "Annotation took time  0:00:12.039969\n",
            "INFO:tensorflow:Writing example 0 of 263\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] [MASK] it is becoming co ##sier to express yourself by different ways . some people do it by using words , some by using pictures of films . but is it clear to allow artist to do and to act how they want ? [SEP] nowadays [SEP]\n",
            "INFO:tensorflow:input_ids: 101 103 2009 2003 3352 2522 20236 2000 4671 4426 2011 2367 3971 1012 2070 2111 2079 2009 2011 2478 2616 1010 2070 2011 2478 4620 1997 3152 1012 2021 2003 2009 3154 2000 3499 3063 2000 2079 1998 2000 2552 2129 2027 2215 1029 102 13367 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] nowadays [MASK] is becoming co ##sier to express yourself by different ways . some people do it by using words , some by using pictures of films . but is it clear to allow artist to do and to act how they want ? [SEP] it [SEP]\n",
            "INFO:tensorflow:input_ids: 101 13367 103 2003 3352 2522 20236 2000 4671 4426 2011 2367 3971 1012 2070 2111 2079 2009 2011 2478 2616 1010 2070 2011 2478 4620 1997 3152 1012 2021 2003 2009 3154 2000 3499 3063 2000 2079 1998 2000 2552 2129 2027 2215 1029 102 2009 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] nowadays it [MASK] becoming co ##sier to express yourself by different ways . some people do it by using words , some by using pictures of films . but is it clear to allow artist to do and to act how they want ? [SEP] is [SEP]\n",
            "INFO:tensorflow:input_ids: 101 13367 2009 103 3352 2522 20236 2000 4671 4426 2011 2367 3971 1012 2070 2111 2079 2009 2011 2478 2616 1010 2070 2011 2478 4620 1997 3152 1012 2021 2003 2009 3154 2000 3499 3063 2000 2079 1998 2000 2552 2129 2027 2215 1029 102 2003 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] nowadays it is [MASK] co ##sier to express yourself by different ways . some people do it by using words , some by using pictures of films . but is it clear to allow artist to do and to act how they want ? [SEP] becoming [SEP]\n",
            "INFO:tensorflow:input_ids: 101 13367 2009 2003 103 2522 20236 2000 4671 4426 2011 2367 3971 1012 2070 2111 2079 2009 2011 2478 2616 1010 2070 2011 2478 4620 1997 3152 1012 2021 2003 2009 3154 2000 3499 3063 2000 2079 1998 2000 2552 2129 2027 2215 1029 102 3352 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: \n",
            "INFO:tensorflow:tokens: [CLS] nowadays it is becoming [MASK] to express yourself by different ways . some people do it by using words , some by using pictures of films . but is it clear to allow artist to do and to act how they want ? [SEP] co ##sier [SEP]\n",
            "INFO:tensorflow:input_ids: 101 13367 2009 2003 3352 103 2000 4671 4426 2011 2367 3971 1012 2070 2111 2079 2009 2011 2478 2616 1010 2070 2011 2478 4620 1997 3152 1012 2021 2003 2009 3154 2000 3499 3063 2000 2079 1998 2000 2552 2129 2027 2215 1029 102 2522 20236 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from OUTPUT_DIR_NAME/model.ckpt-1874\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "\n",
            "\n",
            "Annotation took time  0:00:12.158832\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3w4nJBGMOuz5",
        "colab_type": "code",
        "outputId": "e865f978-f267-4b71-aaa4-029572d260e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Create & upload a file.\n",
        "uploaded = drive.CreateFile({'title': curname})\n",
        "uploaded.SetContentFile('./'+curname)\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 16_pmUKOHS5lKXM2o-1QyVcPQohalCRZd\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}